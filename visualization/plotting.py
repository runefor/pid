from pathlib import Path
from typing import Optional

import dask
import dask.array as da
import dask.dataframe as dd
from dask import compute
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import seaborn as sns
import pandas as pd


plt.style.use('seaborn-v0_8-whitegrid') 

def plot_dask_log_histogram(
    ddf: dd.DataFrame, 
    column: str, 
    title: str,
    xlabel: str,
    ylabel: str = "Frequency",
    bins: int = 50,
    save_path: Optional[Path] = None
):
    """
    Dask ë°ì´í„°í”„ë ˆì„ì˜ íŠ¹ì • ì»¬ëŸ¼ì— ëŒ€í•œ ë¡œê·¸ ìŠ¤ì¼€ì¼ íˆìŠ¤í† ê·¸ë¨ì„ ê·¸ë¦½ë‹ˆë‹¤.

    Args:
        ddf (dd.DataFrame): ëŒ€ìƒ Dask ë°ì´í„°í”„ë ˆì„.
        column (str): íˆìŠ¤í† ê·¸ë¨ì„ ê·¸ë¦´ ì»¬ëŸ¼ëª….
        title (str): í”Œë¡¯ì˜ ì œëª©.
        xlabel (str): xì¶• ë¼ë²¨.
        ylabel (str, optional): yì¶• ë¼ë²¨. Defaults to "Frequency".
        bins (int, optional): íˆìŠ¤í† ê·¸ë¨ì˜ ë¹ˆ ê°œìˆ˜. Defaults to 50.
        save_path (Path, optional): Noneì´ ì•„ë‹ˆë©´ í”Œë¡¯ì„ í•´ë‹¹ ê²½ë¡œì— ì €ì¥. Defaults to None.
        
    Returns:
        tuple: (matplotlib.figure, matplotlib.axes) ê°ì²´ë¥¼ ë°˜í™˜.
    """
    # 1. íˆìŠ¤í† ê·¸ë¨ ê³„ì‚° ê³„íš ì„¸ìš°ê¸°
    col_min, col_max = dask.compute(ddf[column].min(), ddf[column].max())
    
    # ìµœì†Œê°’ì´ 0ì´í•˜ì¼ ê²½ìš° ë¡œê·¸ ìŠ¤ì¼€ì¼ ì˜¤ë¥˜ ë°©ì§€
    start_val = max(col_min, 1e-6) 
    bin_edges_plan = np.logspace(np.log10(start_val), np.log10(col_max), bins)

    counts_plan, bin_edges_plan = da.histogram(ddf[column], bins=bin_edges_plan)

    # 2. ì‹¤ì œ ê³„ì‚° ì‹¤í–‰ (ê²°ê³¼ëŠ” ì‘ì€ numpy ë°°ì—´)
    counts, bin_edges = dask.compute(counts_plan, bin_edges_plan)

    # 3. Matplotlibë¡œ ê²°ê³¼ í”Œë¡œíŒ…
    fig, ax = plt.subplots(figsize=(10, 6))
    
    bin_widths = np.diff(bin_edges)
    ax.bar(bin_edges[:-1], counts, width=bin_widths, align='edge', alpha=0.8)

    ax.set_title(title)
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    ax.set_xscale('log')
    ax.grid(True, which="both", ls="--", linewidth=0.5)

    if save_path:
        save_path.parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"Plot saved to {save_path}")
        plt.close(fig) # íŒŒì¼ì„ ì €ì¥í•œ í›„ì—ëŠ” ì°½ì„ ë‹«ì•„ì¤ë‹ˆë‹¤.
    else:
        plt.show()
        
    return fig, ax


def plot_dask_histogram(
    ddf: dd.DataFrame,
    column: str,
    title: str,
    xlabel: str,
    ylabel: str = "Frequency",
    bins: int = 50,
    show_kde: bool = False,
    save_path: Optional[Path] = None
) -> tuple:
    """
    Dask ë°ì´í„°í”„ë ˆì„ì˜ íŠ¹ì • ì»¬ëŸ¼ì— ëŒ€í•œ ì¼ë°˜ íˆìŠ¤í† ê·¸ë¨ì„ ê·¸ë¦½ë‹ˆë‹¤.
    ì„ íƒì ìœ¼ë¡œ KDE(ë°€ë„ ê³¡ì„ )ë¥¼ í•¨ê»˜ í‘œì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    print(f"'{column}' ì»¬ëŸ¼ì— ëŒ€í•œ íˆìŠ¤í† ê·¸ë¨ ê³„ì‚°ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    # 1. íˆìŠ¤í† ê·¸ë¨ ê³„ì‚° ê³„íš ì„¸ìš°ê¸°
    col_min, col_max = dask.compute(ddf[column].min(), ddf[column].max())
    
    # linspaceëŠ” êµ¬ê°„ ê²½ê³„ë¥¼ ìƒì„±í•˜ë¯€ë¡œ, êµ¬ê°„(bin) ê°œìˆ˜ë³´ë‹¤ 1 ë§ê²Œ ì„¤ì •
    bin_edges_plan = np.linspace(col_min, col_max, bins + 1)
    
    counts_plan, bin_edges_plan = da.histogram(ddf[column], bins=bin_edges_plan)

    # 2. ì‹¤ì œ ê³„ì‚° ì‹¤í–‰
    counts, bin_edges = dask.compute(counts_plan, bin_edges_plan)
    print("ê³„ì‚° ì™„ë£Œ.")

    # 3. Matplotlibë¡œ ê²°ê³¼ í”Œë¡œíŒ…
    print("ì‹œê°í™” ìƒì„± ì¤‘...")
    fig, ax = plt.subplots(figsize=(12, 7))
    
    bin_widths = np.diff(bin_edges)
    ax.bar(bin_edges[:-1], counts, width=bin_widths, align='edge', alpha=0.7, label='Frequency')

    ax.set_title(title, fontsize=16)
    ax.set_xlabel(xlabel, fontsize=12)
    ax.set_ylabel(ylabel, fontsize=12)
    ax.grid(True, which="both", ls="--", linewidth=0.5)

    # 4. KDE ê³¡ì„  ê·¸ë¦¬ê¸° (show_kde=Trueì¼ ê²½ìš°)
    if show_kde:
        # KDEëŠ” ë³„ë„ì˜ yì¶•(ì˜¤ë¥¸ìª½)ì„ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •
        ax2 = ax.twinx()
        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
        sns.kdeplot(x=bin_centers, weights=counts, color='red', ax=ax2, label='Density')
        ax2.set_ylabel('Density', fontsize=12, color='red')
        ax2.tick_params(axis='y', labelcolor='red')
        
        # ë²”ë¡€(legend)ë¥¼ í•©ì³ì„œ í‘œì‹œ
        lines, labels = ax.get_legend_handles_labels()
        lines2, labels2 = ax2.get_legend_handles_labels()
        ax2.legend(lines + lines2, labels + labels2, loc='upper right')

    if save_path:
        save_path.parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"Plot saved to {save_path}")
        plt.close(fig)
    else:
        plt.show()
        
    return fig, ax


def plot_dask_spatial_distribution(
    annotations_ddf: dd.DataFrame,
    images_ddf: dd.DataFrame,
    title: str,
    gridsize: int = 100,
    save_path: Optional[Path] = None
) -> plt.Figure:
    """
    annotationsì™€ images Dask ë°ì´í„°í”„ë ˆì„ì„ ë³‘í•©í•˜ì—¬,
    ì •ê·œí™”ëœ ë°”ìš´ë”© ë°•ìŠ¤ ì¤‘ì‹¬ì ì˜ ê³µê°„ì  ë¶„í¬ë¥¼ 2D íˆìŠ¤í† ê·¸ë¨ìœ¼ë¡œ ê·¸ë¦½ë‹ˆë‹¤.
    """
    print("ë°”ìš´ë”© ë°•ìŠ¤ ê³µê°„ ë¶„í¬ ì‹œê°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # --- 1. ë°ì´í„° ì¤€ë¹„ (ë³‘í•© ë° ì •ê·œí™”) ---
    print("ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë³‘í•©ì„ ìœ„í•´ images ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
    # ì‘ì€ images_ddfë§Œ Pandasë¡œ ë³€í™˜í•˜ì—¬ ë©”ëª¨ë¦¬ì— ë¡œë“œ
    images_pdf = images_ddf[['id', 'width', 'height']].rename(columns={'id': 'image_id'}).compute()
    print("ë¡œë“œ ì™„ë£Œ.")

    # annotationsì˜ ê° íŒŒí‹°ì…˜ì— images_pdfë¥¼ ë³‘í•©í•˜ëŠ” í•¨ìˆ˜
    def merge_and_normalize(partition_df, lookup_df):
        merged = pd.merge(partition_df, lookup_df, on='image_id')
        merged['norm_center_x'] = merged['center_x'] / merged['width']
        merged['norm_center_y'] = merged['center_y'] / merged['height']
        return merged[['norm_center_x', 'norm_center_y']]

    # Daskì—ê²Œ ê²°ê³¼ë¬¼ì˜ ìŠ¤í‚¤ë§ˆ(meta)ë¥¼ ì•Œë ¤ì¤Œ
    meta_df = pd.DataFrame({
        'norm_center_x': pd.Series(dtype='float64'),
        'norm_center_y': pd.Series(dtype='float64'),
    })
    
    # map_partitionsìœ¼ë¡œ ë³‘í•© ë° ì •ê·œí™” ê³„íš ìˆ˜ë¦½
    normalized_coords_ddf = annotations_ddf[['image_id', 'center_x', 'center_y']].map_partitions(
        merge_and_normalize,
        lookup_df=images_pdf,
        meta=meta_df
    )

    # --- 2. 2D íˆìŠ¤í† ê·¸ë¨ ê³„ì‚° ---
    print("2D íˆìŠ¤í† ê·¸ë¨ ê³„ì‚° ì¤‘ (ì—°ì‚° íŠ¸ë¦¬ê±°)...")
    bins = [gridsize, gridsize]
    range_ = [[0, 1], [0, 1]]

    H, xedges, yedges = da.histogram2d(
        normalized_coords_ddf['norm_center_x'].values,
        normalized_coords_ddf['norm_center_y'].values,
        bins=bins,
        range=range_
    )
    computed_H, computed_xedges, computed_yedges = dask.compute(H, xedges, yedges)
    print("ê³„ì‚° ì™„ë£Œ.")

    # --- 3. Matplotlibìœ¼ë¡œ ì‹œê°í™” ---
    print("Matplotlibìœ¼ë¡œ ì‹œê°í™” ìƒì„± ì¤‘...")
    fig = plt.figure(figsize=(9, 9))
    gs = gridspec.GridSpec(4, 4, figure=fig)
    ax_main = fig.add_subplot(gs[1:4, 0:3])
    ax_top = fig.add_subplot(gs[0, 0:3], sharex=ax_main)
    ax_right = fig.add_subplot(gs[1:4, 3], sharey=ax_main)
    
    fig.suptitle(title, y=0.93, fontsize=16)

    # ë©”ì¸ 2D íˆìŠ¤í† ê·¸ë¨
    im = ax_main.pcolormesh(computed_xedges, computed_yedges, computed_H.T, cmap='viridis')
    ax_main.set_xlabel('Normalized Center X')
    ax_main.set_ylabel('Normalized Center Y')

    # ìƒë‹¨ 1D íˆìŠ¤í† ê·¸ë¨
    x_counts = computed_H.sum(axis=1)
    ax_top.bar(computed_xedges[:-1], x_counts, width=np.diff(computed_xedges), align='edge')
    plt.setp(ax_top.get_xticklabels(), visible=False)
    ax_top.set_yticks([])

    # ì˜¤ë¥¸ìª½ 1D íˆìŠ¤í† ê·¸ë¨
    y_counts = computed_H.sum(axis=0)
    ax_right.barh(computed_yedges[:-1], y_counts, height=np.diff(computed_yedges), align='edge')
    plt.setp(ax_right.get_yticklabels(), visible=False)
    ax_right.set_xticks([])

    # yì¶• ë’¤ì§‘ê¸° (ì´ë¯¸ì§€ ì¢Œí‘œê³„ì™€ ë™ì¼í•˜ê²Œ)
    ax_main.invert_yaxis()
    gs.tight_layout(fig, rect=[0, 0, 1, 0.95])
    gs.update(wspace=0.05, hspace=0.05)

    if save_path:
        save_path.parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"Plot saved to {save_path}")
        plt.close(fig)
    else:
        plt.show()
        
    return fig


def plot_dask_feature_distributions(
    ddf: dd.DataFrame,
    features: list[str],
    title: str,
    ncols: int = 2,
    bins: int = 100,
    aspect_ratio_range: Optional[tuple[float, float]] = (0, 5),
    save_path: Optional[Path] = None
) -> plt.Figure:
    """
    Dask ë°ì´í„°í”„ë ˆì„ì˜ ì—¬ëŸ¬ ìˆ«ì ì»¬ëŸ¼(í”¼ì²˜)ì— ëŒ€í•œ ë¶„í¬ë¥¼
    í•˜ë‚˜ì˜ ê·¸ë¦¬ë“œì— íˆìŠ¤í† ê·¸ë¨ìœ¼ë¡œ ê·¸ë¦½ë‹ˆë‹¤.

    Args:
        ddf (dd.DataFrame): ëŒ€ìƒ Dask ë°ì´í„°í”„ë ˆì„.
        features (list[str]): íˆìŠ¤í† ê·¸ë¨ì„ ê·¸ë¦´ ì»¬ëŸ¼ëª… ë¦¬ìŠ¤íŠ¸.
        title (str): ì „ì²´ í”Œë¡¯ì˜ ì œëª©.
        ncols (int, optional): ê·¸ë¦¬ë“œì˜ ì—´ ê°œìˆ˜. Defaults to 2.
        bins (int, optional): ê° íˆìŠ¤í† ê·¸ë¨ì˜ ë¹ˆ ê°œìˆ˜. Defaults to 100.
        aspect_ratio_range (tuple, optional): 'aspect_ratio' ì»¬ëŸ¼ì˜ ë²”ìœ„ë¥¼ ì œí•œ. Noneì´ë©´ ì œí•œ ì—†ìŒ.
        save_path (Optional[Path], optional): Noneì´ ì•„ë‹ˆë©´ í”Œë¡¯ì„ í•´ë‹¹ ê²½ë¡œì— ì €ì¥.
    """
    print("ì—¬ëŸ¬ í”¼ì²˜ì— ëŒ€í•œ íˆìŠ¤í† ê·¸ë¨ ê³„ì‚° ë° ì‹œê°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # 1. ì„œë¸Œí”Œë¡¯ ê·¸ë¦¬ë“œ ìƒì„±
    nrows = (len(features) + ncols - 1) // ncols  # í•„ìš”í•œ í–‰ ê°œìˆ˜ ìë™ ê³„ì‚°
    fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 7, nrows * 5))
    
    # axesê°€ 1ì°¨ì› ë°°ì—´ì´ ë˜ë„ë¡ flatten() ì²˜ë¦¬ (plotì´ 1ê°œì¼ ë•Œë„ ì—ëŸ¬ ë°©ì§€)
    axes = np.array(axes).flatten()

    # 2. ê° í”¼ì²˜ì— ëŒ€í•´ ë°˜ë³µí•˜ë©° íˆìŠ¤í† ê·¸ë¨ ê³„ì‚° ë° ê·¸ë¦¬ê¸°
    for i, feature in enumerate(features):
        ax = axes[i]
        
        # NaN/inf ê°’ ì œì™¸
        valid_data = ddf[feature].dropna()
        valid_data = valid_data[~valid_data.isin([np.inf, -np.inf])]

        min_val, max_val = dask.compute(valid_data.min(), valid_data.max())
        
        # í”¼ì²˜ì— ë”°ë¥¸ ì¡°ê±´ë¶€ bin ìƒì„±
        plot_bins = bins
        if feature == 'area' and min_val > 0:
            plot_bins = np.logspace(np.log10(min_val), np.log10(max_val), bins)
            ax.set_xscale('log')
        else:
            if feature == 'aspect_ratio' and aspect_ratio_range:
                min_val = max(aspect_ratio_range[0], min_val)
                max_val = min(aspect_ratio_range[1], max_val)
            plot_bins = np.linspace(min_val, max_val, bins)

        # Daskë¡œ íˆìŠ¤í† ê·¸ë¨ ê³„ì‚°
        counts, bin_edges = da.histogram(valid_data.values, bins=plot_bins)
        computed_counts, computed_bin_edges = dask.compute(counts, bin_edges)
        
        # ì„œë¸Œí”Œë¡¯ì— ë§‰ëŒ€ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
        ax.bar(
            computed_bin_edges[:-1], 
            computed_counts, 
            width=np.diff(computed_bin_edges), 
            align='edge',
            alpha=0.8
        )
        ax.set_title(f'Distribution of {feature.capitalize()}', fontsize=14)
        ax.set_xlabel(feature)
        ax.set_ylabel('Frequency')
        ax.grid(True, ls="--", linewidth=0.5)

    # ë‚¨ëŠ” ë¹ˆ ì„œë¸Œí”Œë¡¯ì€ ë³´ì´ì§€ ì•Šê²Œ ì²˜ë¦¬
    for j in range(i + 1, len(axes)):
        axes[j].set_visible(False)
        
    fig.suptitle(title, fontsize=20, y=1.02)
    fig.tight_layout()
    
    if save_path:
        save_path.parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"Plot saved to {save_path}")
        plt.close(fig)
    else:
        plt.show()

    return fig


def calculate_dask_boxplot_stats( # TODO: ì´ê±´ ì‹œê°í™” ì½”ë“œê°€ ì•„ë‹ˆê¸´ í•´ì„œ ë¶„ë¦¬ê°€ í•„ìš”í•  ë“¯
    ddf: dd.DataFrame,
    categories_df: pd.DataFrame,
    group_col: str = 'category_id',
    value_col: str = 'area',
    top_n: Optional[int] = 15
) -> pd.DataFrame:
    """
    Dask ë°ì´í„°í”„ë ˆì„ì—ì„œ ë°•ìŠ¤ í”Œë¡¯ì„ ê·¸ë¦¬ê¸° ìœ„í•œ í†µê³„ì¹˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        ddf (dd.DataFrame): annotations Dask ë°ì´í„°í”„ë ˆì„.
        categories_df (pd.DataFrame): ì¹´í…Œê³ ë¦¬ ì´ë¦„ ì •ë³´ê°€ ìˆëŠ” Pandas ë°ì´í„°í”„ë ˆì„.
        group_col (str): ê·¸ë£¹í™”í•  ì»¬ëŸ¼ëª… (ì˜ˆ: 'category_id').
        value_col (str): í†µê³„ì¹˜ë¥¼ ê³„ì‚°í•  ì»¬ëŸ¼ëª… (ì˜ˆ: 'area').
        top_n (int, optional): ìƒìœ„ nê°œ ì¹´í…Œê³ ë¦¬ë§Œ ì„ íƒ. Noneì´ë©´ ì „ì²´.
    
    Returns:
        pd.DataFrame: ë°•ìŠ¤ í”Œë¡¯ í†µê³„ì¹˜ê°€ ê³„ì‚°ëœ Pandas ë°ì´í„°í”„ë ˆì„.
    """
    print(f"'{value_col}'ì— ëŒ€í•œ ë°•ìŠ¤ í”Œë¡¯ í†µê³„ì¹˜ ê³„ì‚°ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # 1. Top N ì¹´í…Œê³ ë¦¬ í•„í„°ë§
    if top_n:
        top_categories = ddf[group_col].value_counts().nlargest(top_n).index.compute()
        filtered_ddf = ddf[ddf[group_col].isin(top_categories)]
    else:
        filtered_ddf = ddf

    # 2. Daskë¡œ Quantile ê³„ì‚° ê³„íš ìˆ˜ë¦½
    stats_series_ddf = filtered_ddf.groupby(group_col)[value_col].apply(
        lambda x: x.quantile([0.25, 0.50, 0.75]),
        meta=pd.Series(dtype='float64', name=value_col)
    )

    # 3. ì‹¤ì œ ê³„ì‚° ì‹¤í–‰ ë° Pandasë¡œ ë³€í™˜
    stats_series_pdf = stats_series_ddf.compute()
    stats_pdf = stats_series_pdf.unstack().rename(columns={0.25: 'q1', 0.50: 'med', 0.75: 'q3'})

    # 4. Whisker ê³„ì‚°
    stats_pdf = pd.merge(stats_pdf, categories_df, left_index=True, right_on='id')
    iqr = stats_pdf['q3'] - stats_pdf['q1']
    stats_pdf['whislo'] = (stats_pdf['q1'] - 1.5 * iqr).clip(lower=0)
    stats_pdf['whishi'] = stats_pdf['q3'] + 1.5 * iqr
    
    print("í†µê³„ì¹˜ ê³„ì‚° ì™„ë£Œ.")
    return stats_pdf


def plot_boxplot_from_stats(
    stats_df: pd.DataFrame,
    title: str,
    xlabel: str,
    ylabel: str,
    ylog: bool = True,
    save_path: Optional[Path] = None
) -> tuple:
    """
    ë¯¸ë¦¬ ê³„ì‚°ëœ í†µê³„ì¹˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë°•ìŠ¤ í”Œë¡¯ì„ ê·¸ë¦½ë‹ˆë‹¤.
    """
    print("ë°•ìŠ¤ í”Œë¡¯ ì‹œê°í™” ìƒì„± ì¤‘...")
    # 1. Matplotlibì˜ bxp í•¨ìˆ˜ì— ë§ëŠ” í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ë³€í™˜
    plot_stats = []
    # ì¤‘ì•™ê°’(med) ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ë³´ê¸° ì¢‹ê²Œ ë§Œë“¬
    sorted_df = stats_df.sort_values('med', ascending=False)
    
    for _, row in sorted_df.iterrows():
        plot_stats.append({
            'label': row['name'], # ì¹´í…Œê³ ë¦¬ ì´ë¦„ ì‚¬ìš©
            'med': row['med'], 'q1': row['q1'], 'q3': row['q3'],
            'whislo': row['whislo'], 'whishi': row['whishi'],
            'fliers': []  # ì´ìƒì¹˜ëŠ” ë”°ë¡œ ê³„ì‚°í•˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ ë¹„ì›Œë‘ 
        })

    # 2. ì‹œê°í™”
    fig, ax = plt.subplots(figsize=(16, 9))
    ax.bxp(plot_stats, showfliers=False, vert=True, patch_artist=True)
    
    if ylog:
        ax.set_yscale('log')
        
    ax.set_title(title, fontsize=16)
    ax.set_xlabel(xlabel, fontsize=12)
    ax.set_ylabel(ylabel, fontsize=12)
    plt.xticks(rotation=45, ha='right')
    fig.tight_layout()

    if save_path:
        save_path.parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(save_path, dpi=150)
        print(f"Plot saved to {save_path}")
        plt.close(fig)
    else:
        plt.show()
        
    return fig, ax

def calculate_object_counts_per_image( # TODO: ì´ê²ƒë„ ê³„ì‚°í•˜ëŠ” ë¶€ë¶„ì¸ë° ë‹¤ë¥¸ìª½ì—ë‹¤ê°€ ë„£ì–´ë‘ë©´ ì¢‹ì„ ê²ƒ ê°™ê¸´í•¨.
    ddf: dd.DataFrame, 
    group_col: str = 'image_id'
) -> pd.Series:
    """
    Dask ë°ì´í„°í”„ë ˆì„ì—ì„œ ì´ë¯¸ì§€ë³„ ê°ì²´ ìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ Pandas Seriesë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print("ì´ë¯¸ì§€ë³„ ê°ì²´ ìˆ˜ ê³„ì‚°ì„ ì‹œì‘í•©ë‹ˆë‹¤ (Dask ì—°ì‚°)...")
    object_counts = ddf[group_col].value_counts()
    object_counts_pd = object_counts.compute()
    print("ê³„ì‚° ì™„ë£Œ.")
    return object_counts_pd

def summarize_object_counts(
    counts_series: pd.Series,
    output_path: Path, # ğŸ‘ˆ print ëŒ€ì‹  íŒŒì¼ ê²½ë¡œë¥¼ ë°›ë„ë¡ ë³€ê²½
    find_single_objects: bool = True
):
    """
    ì´ë¯¸ì§€ë³„ ê°ì²´ ìˆ˜(Pandas Series)ë¥¼ ë°›ì•„ í†µê³„ì¹˜ë¥¼ Markdown ë¦¬í¬íŠ¸ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    print(f"'{output_path}' ê²½ë¡œì— ë¶„ì„ ìš”ì•½ ë¦¬í¬íŠ¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤...")
    
    # 1. Markdown ë‚´ìš©ì„ ë‹´ì„ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    report_lines = []
    
    # 2. ê° ë¶„ì„ ê²°ê³¼ë¥¼ Markdown í˜•ì‹ì˜ ë¬¸ìì—´ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.
    report_lines.append("# ì´ë¯¸ì§€ë³„ ê°ì²´ ìˆ˜ ë¶„ì„ ë¦¬í¬íŠ¸\n\n")
    
    report_lines.append("## í†µê³„ ìš”ì•½\n")
    report_lines.append(f"- **ê³ ìœ  ì´ë¯¸ì§€ ìˆ˜**: {len(counts_series)}\n")
    report_lines.append(f"- **ì´ë¯¸ì§€ ë‹¹ í‰ê·  ê°ì²´ ìˆ˜**: {counts_series.mean():.2f}\n")
    report_lines.append(f"- **ê°€ì¥ ê°ì²´ê°€ ë§ì€ ì´ë¯¸ì§€ì˜ ê°ì²´ ìˆ˜**: {counts_series.max()}\n")
    report_lines.append(f"- **ê°€ì¥ ê°ì²´ê°€ ì ì€ ì´ë¯¸ì§€ì˜ ê°ì²´ ìˆ˜**: {counts_series.min()}\n\n")
    
    max_objects_id = counts_series.idxmax()
    report_lines.append("## ì£¼ìš” ì´ë¯¸ì§€ ì •ë³´\n")
    report_lines.append(f"- **ê°€ì¥ ê°ì²´ê°€ ë§ì€ ì´ë¯¸ì§€ ID**: `{max_objects_id}` (ê°ì²´ ìˆ˜: {counts_series.max()})\n\n")
    max_objects_ids = counts_series.nlargest(5)
    report_lines.append("### ê°€ì¥ ê°ì²´ê°€ ë§ì€ ì´ë¯¸ì§€ë“¤ Top 5\n")
    report_lines.append(max_objects_ids.to_frame(name="Object Count").to_markdown())
    report_lines.append("\n\n")

    min_objects_ids = counts_series.nsmallest(5)
    report_lines.append("### ê°€ì¥ ê°ì²´ê°€ ì ì€ ì´ë¯¸ì§€ë“¤ Top 5\n")
    
    report_lines.append(min_objects_ids.to_frame(name="Object Count").to_markdown())
    report_lines.append("\n\n")
    
    if find_single_objects:
        single_object_images = counts_series[counts_series == 1]
        report_lines.append(f"## ê°ì²´ê°€ 1ê°œì¸ ì´ë¯¸ì§€ ë¶„ì„\n")
        report_lines.append(f"- **ì´ ê°œìˆ˜**: {len(single_object_images)}\n\n")
        report_lines.append("### ID ëª©ë¡ (ìƒìœ„ 100ê°œ)\n")
        
        report_lines.append(single_object_images.head(100).to_frame(name="Object Count").to_markdown())
        report_lines.append("\n")

    # 3. ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹˜ê³  íŒŒì¼ì— ì”ë‹ˆë‹¤.
    report_content = "".join(report_lines)
    
    # ì €ì¥ ê²½ë¡œì˜ ë¶€ëª¨ í´ë”ê°€ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ìƒì„±
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report_content)
        
    print("ë¦¬í¬íŠ¸ ì €ì¥ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")


def plot_object_counts_distribution(
    counts_series: pd.Series,
    title: str,
    xlabel: str = "Number of Objects per Image",
    ylabel: str = "Number of Images",
    bins: int = 100,
    save_path: Optional[Path] = None
) -> tuple:
    """
    ì´ë¯¸ì§€ë³„ ê°ì²´ ìˆ˜ ë¶„í¬ë¥¼ íˆìŠ¤í† ê·¸ë¨ìœ¼ë¡œ ê·¸ë¦½ë‹ˆë‹¤.
    """
    fig, ax = plt.subplots(figsize=(12, 6))
    
    counts_series.hist(ax=ax, bins=bins)
    
    ax.set_title(title, fontsize=16)
    ax.set_xlabel(xlabel, fontsize=12)
    ax.set_ylabel(ylabel, fontsize=12)
    ax.grid(False)
    
    fig.tight_layout()

    if save_path:
        save_path.parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(save_path, dpi=150)
        print(f"Plot saved to {save_path}")
        plt.close(fig)
    else:
        plt.show()
        
    return fig, ax


def analyze_image_resolutions(ddf: dd.DataFrame, n_examples: int = 3) -> tuple[pd.Series, pd.DataFrame, pd.Series, pd.Series, pd.Series]:
    """
    images Dask ë°ì´í„°í”„ë ˆì„ì—ì„œ í•´ìƒë„ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. (ìˆ˜ì •ëœ ë²„ì „)
    """
    print("ì´ë¯¸ì§€ í•´ìƒë„ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤ (Dask ì—°ì‚°)...")
    
    if 'area' not in ddf.columns:
        ddf['area'] = ddf['width'] * ddf['height']
    if 'resolution' not in ddf.columns:
        ddf['resolution'] = ddf['width'].astype(str) + 'x' + ddf['height'].astype(str)
    
    # 1. ìµœê³ /ìµœì € í•´ìƒë„ ì´ë¯¸ì§€ ì •ë³´ë¥¼ ë¨¼ì € í™•ì •í•©ë‹ˆë‹¤.
    print("ìµœê³ /ìµœì € í•´ìƒë„ ì´ë¯¸ì§€ë¥¼ ì°¾ëŠ” ì¤‘...")
    # 1-1. ì¸ë±ìŠ¤ ê°’ì„ ë¨¼ì € ê³„ì‚°
    max_area_image_ddf = ddf.nlargest(1, 'area')
    min_area_image_ddf = ddf.nsmallest(1, 'area')
    

    # 2. ë‚˜ë¨¸ì§€ í†µê³„ì¹˜ë“¤ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    print("ë‚˜ë¨¸ì§€ í†µê³„ì¹˜ë“¤ì„ ê³„ì‚° ì¤‘...")
    resolution_counts = ddf['resolution'].value_counts()
    resolution_stats = ddf[['width', 'height']].describe()
    resolution_id_examples = ddf.groupby('resolution')['id'].apply(
        lambda s: s.head(n_examples).tolist(), meta=('id', 'object')
    )
    
    # ë‚˜ë¨¸ì§€ ì—°ì‚°ë“¤ë§Œ í•œ ë²ˆì— ì‹¤í–‰
    counts_pd, stats_pd, examples_pd, max_area_image_pd, min_area_image_pd = compute(
        resolution_counts, resolution_stats, resolution_id_examples,
        max_area_image_ddf, min_area_image_ddf
    )
    
    counts_pd = counts_pd.sort_values(ascending=False)
    print("ê³„ì‚° ì™„ë£Œ.")
    
    return counts_pd, stats_pd, examples_pd, max_area_image_pd, min_area_image_pd

def save_resolution_summary(
    resolution_counts: pd.Series,
    resolution_stats: pd.DataFrame,
    image_id_examples: pd.Series,
    max_area_image: pd.Series,
    min_area_image: pd.Series,
    output_path: Path,
    top_n: int = 20,
    bottom_n: int = 20,
):
    """
    í•´ìƒë„ ë¶„ì„ ê²°ê³¼ë¥¼ Markdown ë¦¬í¬íŠ¸ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    print(f"'{output_path}' ê²½ë¡œì— í•´ìƒë„ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤...")
    
    # 1. ëª¨ë“  ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ë¨¼ì € í•©ì¹©ë‹ˆë‹¤.
    counts_df = resolution_counts.to_frame(name="Image Count")
    examples_df = image_id_examples.to_frame(name="Example Image IDs")
    
    # joinì„ ì‚¬ìš©í•˜ì—¬ ë‘ ì •ë³´ë¥¼ í•©ì¹©ë‹ˆë‹¤.
    combined_df = counts_df.join(examples_df)
    
    # 2. í•©ì³ì§„ DataFrameì„ 'Image Count' ê¸°ì¤€ìœ¼ë¡œ í™•ì‹¤í•˜ê²Œ ì •ë ¬í•©ë‹ˆë‹¤.
    # ì´ ë‹¨ê³„ê°€ ëª¨ë“  ì •ë ¬ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.
    sorted_df = combined_df.sort_values("Image Count", ascending=False)
    
    # --- 3. ì´ì œ ì •ë ¬ëœ DataFrameì—ì„œ ìƒìœ„/í•˜ìœ„ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. ---
    report_lines = ["# ì´ë¯¸ì§€ í•´ìƒë„ ë¶„ì„ ë¦¬í¬íŠ¸\n\n"]
    
    max_img_row = max_area_image.iloc[0]
    min_img_row = min_area_image.iloc[0]
    
    report_lines.append("## ì£¼ìš” í†µê³„\n")
    report_lines.append(f"- **ê°€ì¥ ë†’ì€ í•´ìƒë„ (ë©´ì  ê¸°ì¤€)**: "
                        f"`{max_img_row['width']}x{max_img_row['height']}` "
                        f"(ID: `{max_img_row['id']}`)\n")
    report_lines.append(f"- **ê°€ì¥ ë‚®ì€ í•´ìƒë„ (ë©´ì  ê¸°ì¤€)**: "
                        f"`{min_img_row['width']}x{min_img_row['height']}` "
                        f"(ID: `{min_img_row['id']}`)\n\n")
    
    report_lines.append("## ë„ˆë¹„ ë° ë†’ì´ í†µê³„\n")
    report_lines.append(resolution_stats.to_markdown())
    report_lines.append("\n\n")
    
    report_lines.append(f"## ê°€ì¥ ë§ì´ ë³´ì´ëŠ” {top_n}ê°œ í•´ìƒë„\n")
    report_lines.append(sorted_df.head(top_n).to_markdown())
    report_lines.append("\n\n")
    
    report_lines.append(f"## ê°€ì¥ ë“œë¬¸ {bottom_n}ê°œ í•´ìƒë„\n")
    # í•˜ìœ„ ë°ì´í„°ëŠ” ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ë³´ì—¬ì£¼ëŠ” ê²ƒì´ ë” ìì—°ìŠ¤ëŸ¬ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    report_lines.append(sorted_df.tail(bottom_n).sort_values("Image Count", ascending=True).to_markdown())
    report_lines.append("\n\n")
    
    report_content = "".join(report_lines)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report_content)
        
    print("ë¦¬í¬íŠ¸ ì €ì¥ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")
    

def plot_top_resolutions(
    resolution_counts_series: pd.Series,
    title: str,
    n_items: int = 20,
    plot_top: bool = True,
    save_path: Optional[Path] = None,
) -> tuple:
    """
    ìƒìœ„ nê°œ í•´ìƒë„ ë¶„í¬ë¥¼ ìˆ˜í‰ ë§‰ëŒ€ê·¸ë˜í”„ë¡œ ê·¸ë¦½ë‹ˆë‹¤.
    """
    if plot_top:
        data_to_plot = resolution_counts_series.head(n_items)
    else:
        # í•˜ìœ„ nê°œëŠ” ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•´ì•¼ ë³´ê¸° ì¢‹ìŠµë‹ˆë‹¤.
        data_to_plot = resolution_counts_series.tail(n_items).sort_values(ascending=True)

    fig, ax = plt.subplots(figsize=(12, 8))
    
    sns.barplot(x=data_to_plot.values, y=data_to_plot.index, orient='h', palette='viridis', ax=ax)
    
    ax.set_title(title, fontsize=16)
    ax.set_xlabel('Number of Images', fontsize=12)
    ax.set_ylabel('Resolution (Width x Height)', fontsize=12)
    
    for index, value in enumerate(data_to_plot):
        ax.text(value, index, f' {value}', va='center')

    fig.tight_layout()

    if save_path:
        save_path.parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(save_path, dpi=150)
        print(f"Plot saved to {save_path}")
        plt.close(fig)
    else:
        plt.show()
        
    return fig, ax


def analyze_class_imbalance(ddf: dd.DataFrame) -> dict: # TODO: ì´ í•¨ìˆ˜ë„ ê³„ì‚°í•˜ëŠ” ë¶€ë¶„ì¸ë° ë‹¤ë¥¸ìª½ì—ë‹¤ê°€ ë„£ì–´ë‘ë©´ ì¢‹ì„ ê²ƒ ê°™ê¸´í•¨.
    """
    í´ë˜ìŠ¤ ê°œìˆ˜ ì‹œë¦¬ì¦ˆë¥¼ ë°›ì•„ ë¶ˆê· í˜• ê´€ë ¨ í†µê³„ì¹˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    print("í´ë˜ìŠ¤ ë¶ˆê· í˜• í†µê³„ì¹˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤...")
    
    # 1. Daskë¡œ í´ë˜ìŠ¤ë³„ ê°œìˆ˜ ê³„ì‚° (ê³„íš)
    class_counts: pd.DataFrame = ddf['category_id'].value_counts().compute()
    
    # 2. í•„ìš”í•œ ëª¨ë“  í†µê³„ì¹˜ ê³„ì‚°ì„ Dask ì—°ì‚°ìœ¼ë¡œ ê³„íš
    tasks = {
        'total_classes': class_counts.shape[0],
        'min_count': class_counts.min(),
        'max_count': class_counts.max(),
        'mean_count': class_counts.mean(),
        'median_count': class_counts.median(),
    }
    
    # 3. ëª¨ë“  ê³„íšì„ í•œ ë²ˆì— ì‹¤í–‰
    results = dask.compute(tasks)[0]
    
    # 4. ê³„ì‚°ëœ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ê°€ ì •ë³´ ìƒì„±
    results['max_min_ratio'] = results['max_count'] / results['min_count'] if results['min_count'] > 0 else float('inf')
    
    # 5. ëˆ„ì  ë¶„í¬ ê³„ì‚° (ì´ ë¶€ë¶„ì€ ê³„ì‚°ëœ Pandas Seriesë¡œ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ ë” íš¨ìœ¨ì )
    cumulative_percentage = class_counts.cumsum() / class_counts.sum()
    results['classes_for_50_percent'] = (cumulative_percentage < 0.50).sum() + 1
    results['classes_for_80_percent'] = (cumulative_percentage < 0.80).sum() + 1
    results['classes_for_95_percent'] = (cumulative_percentage < 0.95).sum() + 1
    
    print("ê³„ì‚° ì™„ë£Œ.")
    return results

def find_non_overlapping_classes(
    train_ddf: dd.DataFrame,
    test_ddf: dd.DataFrame,
    categories_df: pd.DataFrame
) -> dict:
    """
    Trainê³¼ Test ë°ì´í„°ì…‹ ê°„ì— ì„œë¡œ ê²¹ì¹˜ì§€ ì•ŠëŠ” í´ë˜ìŠ¤ë¥¼ ì°¾ì•„ì„œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print("Train/Test ë°ì´í„°ì…‹ ê°„ í´ë˜ìŠ¤ ID ëª©ë¡ì„ ë¹„êµí•©ë‹ˆë‹¤...")
    
    # 1. ê° ë°ì´í„°ì…‹ì˜ ê³ ìœ  ì¹´í…Œê³ ë¦¬ ID ëª©ë¡ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    train_ids_task = train_ddf['category_id'].unique()
    test_ids_task = test_ddf['category_id'].unique()
    
    train_ids_pd, test_ids_pd = compute(train_ids_task, test_ids_task)
    
    # 2. set(ì§‘í•©)ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì°¨ì§‘í•©ì„ êµ¬í•©ë‹ˆë‹¤.
    train_id_set = set(train_ids_pd)
    test_id_set = set(test_ids_pd)
    
    train_only_ids = train_id_set - test_id_set
    test_only_ids = test_id_set - train_id_set
    
    # 3. IDë¥¼ ì‹¤ì œ í´ë˜ìŠ¤ ì´ë¦„ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    category_map = pd.Series(categories_df['name'].values, index=categories_df.index).to_dict()
    
    
    train_only_names = [category_map.get(id, f"Unknown ID: {id}") for id in train_only_ids]
    test_only_names = [category_map.get(id, f"Unknown ID: {id}") for id in test_only_ids]
    
    print("ë¹„êµ ì™„ë£Œ.")
    
    return {
        'train_only': sorted(train_only_names),
        'test_only': sorted(test_only_names)
    }

def save_combined_imbalance_report(
    train_stats: dict,
    test_stats: dict,
    non_overlapping_info: dict,
    output_path: Path
):
    """
    Train/Test í´ë˜ìŠ¤ ë¶ˆê· í˜• í†µê³„ì¹˜ì™€ ê²¹ì¹˜ì§€ ì•ŠëŠ” í´ë˜ìŠ¤ ì •ë³´ë¥¼
    í•˜ë‚˜ì˜ ì¢…í•© Markdown ë¦¬í¬íŠ¸ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    print(f"'{output_path}' ê²½ë¡œì— ì¢…í•© í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¦¬í¬íŠ¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤...")

    # ì½ê¸° ì‰¬ìš´ ë¼ë²¨ ë§¤í•‘
    key_labels = {
        'total_classes': 'ì´ í´ë˜ìŠ¤ ìˆ˜',
        'min_count': 'ìµœì†Œ ê°ì²´ ìˆ˜',
        'max_count': 'ìµœëŒ€ ê°ì²´ ìˆ˜',
        'mean_count': 'í‰ê·  ê°ì²´ ìˆ˜',
        'median_count': 'ê°ì²´ ìˆ˜ ì¤‘ì•™ê°’',
        'max_min_ratio': 'ìµœëŒ€/ìµœì†Œ í´ë˜ìŠ¤ ë¹„ìœ¨',
        'classes_for_50_percent': 'ë°ì´í„°ì˜ 50%ë¥¼ ì°¨ì§€í•˜ëŠ” ìƒìœ„ í´ë˜ìŠ¤ ìˆ˜',
        'classes_for_80_percent': 'ë°ì´í„°ì˜ 80%ë¥¼ ì°¨ì§€í•˜ëŠ” ìƒìœ„ í´ë˜ìŠ¤ ìˆ˜',
        'classes_for_95_percent': 'ë°ì´í„°ì˜ 95%ë¥¼ ì°¨ì§€í•˜ëŠ” ìƒìœ„ í´ë˜ìŠ¤ ìˆ˜',
    }
    
    # --- ë¦¬í¬íŠ¸ ë‚´ìš© ìƒì„± ---
    report_lines = ["# Train vs Test í´ë˜ìŠ¤ ë¶ˆê· í˜• ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸\n\n"]
    
    # Train ë°ì´í„° í†µê³„ì¹˜ ì„¹ì…˜
    report_lines.append("## Train ë°ì´í„°ì…‹ ë¶ˆê· í˜• ì§€í‘œ\n")
    for key, value in train_stats.items():
        label = key_labels.get(key, key)
        formatted_value = f"{value:,.2f}" if isinstance(value, float) else f"{value:,}"
        report_lines.append(f"- **{label}**: {formatted_value}\n")
    report_lines.append("\n")

    # Test ë°ì´í„° í†µê³„ì¹˜ ì„¹ì…˜
    report_lines.append("## Test ë°ì´í„°ì…‹ ë¶ˆê· í˜• ì§€í‘œ\n")
    for key, value in test_stats.items():
        label = key_labels.get(key, key)
        formatted_value = f"{value:,.2f}" if isinstance(value, float) else f"{value:,}"
        report_lines.append(f"- **{label}**: {formatted_value}\n")
    report_lines.append("\n")
    
    # ê²¹ì¹˜ì§€ ì•ŠëŠ” í´ë˜ìŠ¤ ë¶„ì„ ì„¹ì…˜
    report_lines.append("## ë°ì´í„°ì…‹ ê°„ í´ë˜ìŠ¤ ë¹„êµ\n")
    train_only = non_overlapping_info['train_only']
    test_only = non_overlapping_info['test_only']
    
    report_lines.append(f"- **Train ë°ì´í„°ì…‹ì—ë§Œ ì¡´ì¬í•˜ëŠ” í´ë˜ìŠ¤**: {len(train_only)}ê°œ\n")
    if train_only:
        report_lines.append("  - " + ", ".join(f"`{name}`" for name in train_only) + "\n")
        
    report_lines.append(f"- **Test ë°ì´í„°ì…‹ì—ë§Œ ì¡´ì¬í•˜ëŠ” í´ë˜ìŠ¤**: {len(test_only)}ê°œ\n")
    if test_only:
        report_lines.append("  - " + ", ".join(f"`{name}`" for name in test_only) + "\n")

    # --- íŒŒì¼ ì €ì¥ ---
    report_content = "".join(report_lines)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report_content)
        
    print("ë¦¬í¬íŠ¸ ì €ì¥ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")


def plot_class_distribution(
    annotations_ddf: dd.DataFrame,
    categories_df:pd.DataFrame,
    title: str,
    top_n: Optional[int] = None,
    show_counts: bool = True,
    x_log_scale: bool = False,
    save_path: Optional[Path] = None
) -> tuple:
    """
    í´ë˜ìŠ¤ë³„ ì–´ë…¸í…Œì´ì…˜ ê°œìˆ˜ ë¶„í¬ë¥¼ ìˆ˜í‰ ë§‰ëŒ€ê·¸ë˜í”„ë¡œ ê·¸ë¦½ë‹ˆë‹¤.

    Args:
        annotations_df (dd.DataFrame): annotations ë°ì´í„°í”„ë ˆì„.
        categories_df (pd.DataFrame): ì¹´í…Œê³ ë¦¬ ì •ë³´ê°€ ìˆëŠ” Pandas ë°ì´í„°í”„ë ˆì„.
        title (str): í”Œë¡¯ì˜ ì œëª©.
        top_n (int, optional): ìƒìœ„ nê°œ í´ë˜ìŠ¤ë§Œ í‘œì‹œ. Noneì´ë©´ ì „ì²´ í‘œì‹œ.
        show_counts (bool, optional): ë§‰ëŒ€ ì˜†ì— ê°œìˆ˜ë¥¼ í‘œì‹œí• ì§€ ì—¬ë¶€.
        save_path (Optional[Path], optional): Noneì´ ì•„ë‹ˆë©´ í”Œë¡¯ì„ í•´ë‹¹ ê²½ë¡œì— ì €ì¥.
    """
    print("í´ë˜ìŠ¤ ë¶„í¬ ì‹œê°í™”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
    # 1. Daskë¡œ í´ë˜ìŠ¤ë³„ ê°œìˆ˜ ê³„ì‚° (ê³„íš)
    class_counts_ddf = annotations_ddf['category_id'].value_counts()

    # 2. ìƒìœ„ Nê°œë§Œ ì„ íƒ (ê³„íš)
    if top_n:
        class_counts_ddf = class_counts_ddf.nlargest(n=top_n)
    
    # 3. Dask Mergeë¥¼ ì‚¬ìš©í•´ ì¹´í…Œê³ ë¦¬ ì´ë¦„ ê²°í•© (ê³„íš)
    # value_counts ê²°ê³¼(Series)ë¥¼ mergeë¥¼ ìœ„í•´ DataFrameìœ¼ë¡œ ë³€í™˜
    counts_to_merge_ddf = class_counts_ddf.to_frame(name='count')
    # categories_dfì˜ 'id'ì™€ counts_ddfì˜ ì¸ë±ìŠ¤(category_id)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ merge
    merged_ddf = counts_to_merge_ddf.merge(
        categories_df,
        left_index=True,
        right_index=True
    )

    # 4. ì‹¤ì œ ê³„ì‚° ì‹¤í–‰ (ì‹œê°í™”ì— í•„ìš”í•œ ì‘ì€ ë°ì´í„°ë§Œ Pandasë¡œ ë³€í™˜)
    plot_df = merged_ddf.compute()
    print("ê³„ì‚° ì™„ë£Œ.")

    # 5. ì‹œê°í™”ë¥¼ ìœ„í•´ ë°ì´í„° ì •ë ¬ ë° ì¸ë±ìŠ¤ ì„¤ì •
    plot_df = plot_df.set_index('name').sort_values('count', ascending=False)
    
    # 6. ì‹œê°í™” (ì´í›„ ë¡œì§ì€ ì´ì „ê³¼ ê±°ì˜ ë™ì¼)
    plot_height = max(8, len(plot_df) * 0.4)
    fig, ax = plt.subplots(figsize=(12, plot_height))
    
    sns.barplot(x=plot_df['count'], y=plot_df.index, orient='h', palette='viridis', ax=ax)
    
    if x_log_scale:
        ax.set_xscale('log')
    
    ax.set_title(title, fontsize=16)
    ax.set_xlabel('Number of Annotations', fontsize=12)
    ax.set_ylabel('Class Name', fontsize=12)

    if show_counts:
        for index, value in enumerate(plot_df['count']):
            ax.text(value, index, f' {value}', va='center', fontsize=10)

    fig.tight_layout()

    if save_path:
        save_path.parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(save_path, dpi=150)
        print(f"Plot saved to {save_path}")
        plt.close(fig)
    else:
        plt.show()
        
    return fig, ax

def plot_top_bottom_classes(
    annotations_ddf: dd.DataFrame,
    categories_df: pd.DataFrame,
    n_items: int = 20,
    save_path: Optional[Path] = None
):
    """ìƒìœ„ Nê°œì™€ í•˜ìœ„ Nê°œ í´ë˜ìŠ¤ ë¶„í¬ë¥¼ í•¨ê»˜ ê·¸ë¦½ë‹ˆë‹¤."""
    # 1. Daskë¡œ ê³„ì‚° ê³„íš ìˆ˜ë¦½
    class_counts = annotations_ddf['category_id'].value_counts()
    top_n = class_counts.nlargest(n=n_items).to_frame(name='count')
    bottom_n = class_counts.nsmallest(n=n_items).to_frame(name='count')
    
    top_merged = top_n.merge(categories_df, left_index=True, right_index=True)
    bottom_merged = bottom_n.merge(categories_df, left_index=True, right_index=True)
    
    # 2. ì‹¤ì œ ê³„ì‚° ì‹¤í–‰
    top_df, bottom_df = dask.compute(top_merged, bottom_merged)
    print("ê³„ì‚° ì™„ë£Œ.")

    # 3. ì‹œê°í™”
    top_df = top_df.set_index('name').sort_values('count', ascending=False)
    bottom_df = bottom_df.set_index('name').sort_values('count', ascending=True)

    fig, axes = plt.subplots(1, 2, figsize=(20, max(8, n_items * 0.4)))
    
    sns.barplot(x=top_df['count'], y=top_df.index, palette='viridis', ax=axes[0])
    axes[0].set_title(f'Top {n_items} Most Frequent Classes')
    axes[0].set_xlabel('Number of Annotations')
    
    sns.barplot(x=bottom_df['count'], y=bottom_df.index, palette='viridis_r', ax=axes[1])
    axes[1].set_title(f'Bottom {n_items} Least Frequent Classes')
    axes[1].set_xlabel('Number of Annotations')
    axes[1].yaxis.tick_right()
    
    fig.tight_layout()
    if save_path:
        save_path.parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(save_path, dpi=150)
        print(f"Plot saved to {save_path}")
        plt.close(fig)
    else:
        plt.show()
    return fig, axes


def plot_cumulative_class_distribution(
    ddf: dd.DataFrame,
    save_path: Optional[Path] = None
):
    """í´ë˜ìŠ¤ ëˆ„ì  ë¶„í¬ ê³¡ì„ ì„ ê·¸ë¦½ë‹ˆë‹¤."""
    print("ëˆ„ì  ë¶„í¬ ê³„ì‚° ì¤‘...")
    class_counts = ddf['category_id'].value_counts().compute()
    print("ê³„ì‚° ì™„ë£Œ.")

    cumulative_dist = class_counts.cumsum() / class_counts.sum() * 100
    
    fig, ax = plt.subplots(figsize=(10, 7))
    ax.plot(np.arange(1, len(cumulative_dist) + 1), cumulative_dist.values)
    
    # 80% ì§€ì  ë“± ì¤‘ìš” ì§€í‘œ í‘œì‹œ
    classes_for_80_percent = (cumulative_dist < 80).sum() + 1
    ax.axhline(80, color='r', linestyle='--', label='80% of Data')
    ax.axvline(classes_for_80_percent, color='r', linestyle='--')
    
    ax.set_title('Cumulative Distribution of Annotations by Class')
    ax.set_xlabel('Number of Classes (sorted by frequency)')
    ax.set_ylabel('Cumulative Percentage of Annotations (%)')
    ax.grid(True)
    ax.legend()
    if save_path:
        save_path.parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(save_path, dpi=150)
        print(f"Plot saved to {save_path}")
        plt.close(fig)
    else:
        plt.show()
    return fig, ax


def calculate_dask_boxplot_stats(
    ddf: dd.DataFrame,
    categories_df: pd.DataFrame,
    group_col: str = 'category_id',
    value_col: str = 'area',
    top_n: Optional[int] = None
) -> pd.DataFrame:
    """
    Dask ë°ì´í„°í”„ë ˆì„ì—ì„œ ë°•ìŠ¤ í”Œë¡¯ì„ ê·¸ë¦¬ê¸° ìœ„í•œ í†µê³„ì¹˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        ...
        top_n (int, optional): ìƒìœ„ nê°œ ì¹´í…Œê³ ë¦¬ë§Œ ì„ íƒí•©ë‹ˆë‹¤. 
                               Noneìœ¼ë¡œ ì§€ì •í•˜ë©´ ì „ì²´ ì¹´í…Œê³ ë¦¬ë¥¼ ëŒ€ìƒìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    print(f"'{value_col}'ì— ëŒ€í•œ ë°•ìŠ¤ í”Œë¡¯ í†µê³„ì¹˜ ê³„ì‚°ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # top_nì´ ì •ìˆ˜ ê°’ì¼ ë•Œë§Œ í•„í„°ë§ì´ ì‹¤í–‰ë©ë‹ˆë‹¤.
    # Noneì´ë©´ ì´ ë¸”ë¡ì„ ê±´ë„ˆë›°ê³  ì „ì²´ ddfë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    if top_n:
        top_categories = ddf[group_col].value_counts().nlargest(top_n).index.compute()
        filtered_ddf = ddf[ddf[group_col].isin(top_categories)]
    else:
        print("ì „ì²´ ì¹´í…Œê³ ë¦¬ë¥¼ ëŒ€ìƒìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.")
        filtered_ddf = ddf

    # 2. Daskë¡œ Quantile ê³„ì‚°
    stats_series_ddf = filtered_ddf.groupby(group_col)[value_col].apply(
        lambda x: x.quantile([0.25, 0.50, 0.75]),
        meta=pd.Series(dtype='float64', name=value_col)
    )
    stats_series_pdf = stats_series_ddf.compute()
    stats_pdf = stats_series_pdf.unstack().rename(columns={0.25: 'q1', 0.50: 'med', 0.75: 'q3'})

    # 3. Whisker (ìˆ˜ì—¼) ê³„ì‚°
    stats_pdf = pd.merge(stats_pdf, categories_df, left_index=True, right_on='id')
    iqr = stats_pdf['q3'] - stats_pdf['q1']
    stats_pdf['whislo'] = (stats_pdf['q1'] - 1.5 * iqr).clip(lower=0)
    stats_pdf['whishi'] = stats_pdf['q3'] + 1.5 * iqr
    
    print("í†µê³„ì¹˜ ê³„ì‚° ì™„ë£Œ.")
    return stats_pdf

def plot_boxplot_from_stats(
    stats_df: pd.DataFrame,
    title: str,
    xlabel: str,
    ylabel: str,
    xlog: bool = True,
    save_path: Optional[Path] = None
) -> tuple:
    """
    ë¯¸ë¦¬ ê³„ì‚°ëœ í†µê³„ì¹˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ê°€ë¡œ ë°©í–¥ì˜ ë°•ìŠ¤ í”Œë¡¯ì„ ê·¸ë¦½ë‹ˆë‹¤.
    """
    print("ë°•ìŠ¤ í”Œë¡¯ ì‹œê°í™” ìƒì„± ì¤‘...")
    
    plot_stats = []
    # ì¤‘ì•™ê°’(med) ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ë³´ê¸° ì¢‹ê²Œ ë§Œë“¬
    sorted_df = stats_df.sort_values('med', ascending=True) # ê°€ë¡œ ë°©í–¥ì´ë¯€ë¡œ ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ë³€ê²½
    
    for _, row in sorted_df.iterrows():
        plot_stats.append({
            'label': row['name'],
            'med': row['med'], 'q1': row['q1'], 'q3': row['q3'],
            'whislo': row['whislo'], 'whishi': row['whishi'],
            'fliers': []
        })

    # 1. ì¹´í…Œê³ ë¦¬ ìˆ˜ì— ë”°ë¼ ê·¸ë˜í”„ ë†’ì´ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì ˆ
    num_categories = len(stats_df)
    plot_height = max(10, num_categories * 0.3) # ì¹´í…Œê³ ë¦¬ë‹¹ 0.3ì¸ì¹˜ í• ë‹¹ (ìµœì†Œ 10ì¸ì¹˜)
    
    fig, ax = plt.subplots(figsize=(16, plot_height))
    
    # 2. vert=False ì˜µì…˜ìœ¼ë¡œ ê°€ë¡œ ë°•ìŠ¤ í”Œë¡¯ ìƒì„±
    ax.bxp(plot_stats, showfliers=False, vert=False, patch_artist=True)
    
    # 3. xì¶•ì„ ë¡œê·¸ ìŠ¤ì¼€ì¼ë¡œ ë³€ê²½
    if xlog:
        ax.set_xscale('log')
        
    ax.set_title(title, fontsize=16)
    ax.set_xlabel(xlabel, fontsize=12) # x ë¼ë²¨
    ax.set_ylabel(ylabel, fontsize=12) # y ë¼ë²¨
    ax.grid(True, axis='x', ls="--", linewidth=0.5) # xì¶•ì—ë§Œ ê·¸ë¦¬ë“œ í‘œì‹œ

    fig.tight_layout()

    if save_path:
        save_path.parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(save_path, dpi=150)
        print(f"Plot saved to {save_path}")
        plt.close(fig)
    else:
        plt.show()
        
    return fig, ax

def plot_dask_wh_scatter(
    ddf: dd.DataFrame,
    title: str,
    sample_frac: float = 0.1, # 10%ë§Œ ìƒ˜í”Œë§í•˜ì—¬ ì‚¬ìš© (ë°ì´í„°ê°€ í¬ë©´ ë” ì¤„ì´ì„¸ìš”)
    alpha: float = 0.3,
    save_path: Optional[Path] = None
) -> tuple:
    """
    Dask ë°ì´í„°í”„ë ˆì„ì˜ ë„ˆë¹„(w)ì™€ ë†’ì´(h)ë¥¼ 2D ì‚°ì ë„(scatter plot)ë¡œ ê·¸ë¦½ë‹ˆë‹¤.
    ë°ì´í„°ê°€ í´ ê²½ìš° ì¼ë¶€ë§Œ ìƒ˜í”Œë§í•˜ì—¬ ì‹œê°í™”í•©ë‹ˆë‹¤.
    """
    print(f"ì „ì²´ ë°ì´í„°ì˜ {sample_frac*100:.0f}%ë¥¼ ìƒ˜í”Œë§í•˜ì—¬ ë„ˆë¹„/ë†’ì´ ë¶„í¬ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤...")
    
    # 1. ë°ì´í„° ìƒ˜í”Œë§ ë° ê³„ì‚°
    if sample_frac < 1.0:
        sampled_ddf = ddf[['w', 'h']].sample(frac=sample_frac)
    else:
        sampled_ddf = ddf[['w', 'h']]
    
    plot_df = sampled_ddf.compute()
    print("ê³„ì‚° ì™„ë£Œ.")

    # 2. ì‹œê°í™”
    fig, ax = plt.subplots(figsize=(10, 10))
    sns.scatterplot(data=plot_df, x='w', y='h', alpha=alpha, ax=ax, edgecolor='none')
    
    ax.set_title(title, fontsize=16)
    ax.set_xlabel('Width', fontsize=12)
    ax.set_ylabel('Height', fontsize=12)
    ax.grid(True, ls="--", linewidth=0.5)
    
    fig.tight_layout()

    if save_path:
        save_path.parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(save_path, dpi=150)
        print(f"Plot saved to {save_path}")
        plt.close(fig)
    else:
        plt.show()
    return fig, ax