import json
import os
from pathlib import Path
from collections import defaultdict, Counter

try:
    from utils.file_utils import load_json_data
except ModuleNotFoundError:
    import sys
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œë¥¼ sys.pathì— ì¶”ê°€í•©ë‹ˆë‹¤.
    project_root_path = Path(__file__).resolve().parents[1]
    sys.path.append(str(project_root_path))
    print(f"[Warning] Added '{project_root_path}' to sys.path for direct execution.")
    
    from utils.file_utils import load_json_data

base_dir = Path(os.getcwd()).resolve()
data_path = base_dir / "assets"
json_root_path = data_path / "preprocessed_data_json"

# ê²°ê³¼ë¥¼ ì €ì¥í•  ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ê²½ë¡œ
output_md_path = data_path / "class_distribution_report.md"

# 1. categories.json ë¡œë“œ
with (data_path / "categories.json").open("r", encoding="utf-8") as f:
    categories = json.load(f)

# category_id â†’ name ë§¤í•‘
id_to_name = {cat["id"]: cat["name"] for cat in categories}

# category_id â†’ ëŒ€ë¶„ë¥˜ ë§¤í•‘
id_to_major = {}
for cat in categories:
    name = cat.get("name", "")
    if "@" in name:
        major = name.split("@")[0]
    else:
        major = "Unknown"
    id_to_major[cat["id"]] = major

# 2. ê°ì²´ ìˆ˜ ì§‘ê³„

def class_counts(json_paths: list[Path]) -> tuple[Counter, dict]:
    annotation_counts = Counter()
    class_to_images = defaultdict(set)

    for json_file in json_paths:
        data = load_json_data(json_file)
        # ê° ì–´ë…¸í…Œì´ì…˜ì—ì„œ ì§ì ‘ image_idë¥¼ ê°€ì ¸ì™€ì•¼ í•©ë‹ˆë‹¤.
        # ì´ë ‡ê²Œ í•˜ë©´ ë³‘í•©ëœ íŒŒì¼ê³¼ ê°œë³„ íŒŒì¼ ëª¨ë‘ì—ì„œ ì˜¬ë°”ë¥´ê²Œ ë™ì‘í•©ë‹ˆë‹¤.
        for ann in data.get("annotations", []):
            cat_id = ann["category_id"]
            image_id = ann["image_id"]
            annotation_counts[cat_id] += 1
            class_to_images[cat_id].add(image_id)
    return annotation_counts, class_to_images


json_paths = list(json_root_path.glob("TL_prepro/TL_*_*/*.json"))
json_path = [data_path / "merged_dataset.json"]

annotation_counts, class_to_images = class_counts(json_paths=json_paths)
# annotation_counts, class_to_images = class_counts(json_paths=json_path)

# 3. ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
report_lines = ["# ğŸ“Š ë°ì´í„°ì…‹ ê°ì²´ ë¶„í¬ ë¶„ì„ ë¦¬í¬íŠ¸\n"]

# 4. ì „ì²´ ê°ì²´ ìˆ˜
total_objects = sum(annotation_counts.values())
report_lines.append(f"## ğŸ”¢ ì „ì²´ ê°ì²´ ìˆ˜: **{total_objects:,}** ê°œ\n")

# 5. ëŒ€ë¶„ë¥˜ ê¸°ì¤€ ê°ì²´ ìˆ˜ ë° ë¹„ìœ¨
major_counts = defaultdict(int)
for cat_id, count in annotation_counts.items():
    major = id_to_major.get(cat_id, "Unknown")
    major_counts[major] += count

report_lines.append("## ğŸ“‚ ëŒ€ë¶„ë¥˜ ê¸°ì¤€ ê°ì²´ ìˆ˜ ë° ë¹„ìœ¨\n")
report_lines.append("| ëŒ€ë¶„ë¥˜ | ê°ì²´ ìˆ˜ | ë¹„ìœ¨ (%) |")
report_lines.append("|:---|---:|---:|")
for major, count in sorted(major_counts.items(), key=lambda x: x[1], reverse=True):
    ratio = count / total_objects * 100
    report_lines.append(f"| {major} | {count:,} | {ratio:.2f}% |")
report_lines.append("\n")

# 6. í´ë˜ìŠ¤ ì§€ì—­ì„± ë¶„ì„ (ì§‘ì¤‘ë„)
locality_data = []
for cat_id, total_anns in annotation_counts.items():
    name = id_to_name.get(cat_id, f"Unknown({cat_id})")
    unique_images = len(class_to_images.get(cat_id, set()))
    avg_per_image = total_anns / unique_images if unique_images > 0 else 0
    locality_data.append((name, total_anns, unique_images, avg_per_image))

# ì´ë¯¸ì§€ ë‹¹ í‰ê·  ê°ì²´ ìˆ˜ê°€ ë†’ì€ ìˆœ (ì§‘ì¤‘ë„ê°€ ë†’ì€ ìˆœ)ìœ¼ë¡œ ì •ë ¬
locality_data.sort(key=lambda x: x[3], reverse=True)

report_lines.append("## ğŸ”¬ í´ë˜ìŠ¤ ì§€ì—­ì„± ë¶„ì„ (ì§‘ì¤‘ë„ ìˆœ)\n")
report_lines.append("`ì´ë¯¸ì§€ ë‹¹ í‰ê·  ê°ì²´ ìˆ˜`ê°€ ë†’ì„ìˆ˜ë¡ ì†Œìˆ˜ì˜ ì´ë¯¸ì§€ì— ê°ì²´ë“¤ì´ ì§‘ì¤‘ë˜ì–´ ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n")
report_lines.append("| ì†Œë¶„ë¥˜ (ì¹´í…Œê³ ë¦¬ëª…) | ì´ ì–´ë…¸í…Œì´ì…˜ ìˆ˜ | ê³ ìœ  ì´ë¯¸ì§€ ìˆ˜ | ì´ë¯¸ì§€ ë‹¹ í‰ê·  ê°ì²´ ìˆ˜ |")
report_lines.append("|:---|---:|---:|---:|")
for name, total_anns, unique_images, avg_per_image in locality_data:
    report_lines.append(f"| {name} | {total_anns:,} | {unique_images:,} | **{avg_per_image:.2f}** |")
report_lines.append("\n")


# 6. í´ë˜ìŠ¤ë³„ ë¹„ìœ¨ ì¶œë ¥ (ì†Œë¶„ë¥˜ ê¸°ì¤€)
report_lines.append("## ğŸ“Š ì†Œë¶„ë¥˜ ê¸°ì¤€ ê°ì²´ ìˆ˜ ë° ë¹„ìœ¨ (ì „ì²´)\n")
report_lines.append("| ì†Œë¶„ë¥˜ (ì¹´í…Œê³ ë¦¬ëª…) | ê°ì²´ ìˆ˜ | ë¹„ìœ¨ (%) |")
report_lines.append("|:---|---:|---:|")
for cat_id, count in annotation_counts.most_common():
    name = id_to_name.get(cat_id, f"Unknown({cat_id})")
    ratio = count / total_objects * 100
    report_lines.append(f"| {name} | {count:,} | {ratio:.2f}% |")
report_lines.append("\n")

# 7. ì „ì²´ ì†Œë¶„ë¥˜ ëª©ë¡ (ëŒ€ë¶„ë¥˜ë³„)
major_to_minors = defaultdict(list)
for cat in categories:
    name = cat.get("name", "")
    if "@" in name:
        major, minor = name.split("@", 1)
        major_to_minors[major].append(minor)
    else:
        major_to_minors["Unknown"].append(name)

report_lines.append("## ğŸ“‹ ì „ì²´ ì†Œë¶„ë¥˜ ëª©ë¡ (ëŒ€ë¶„ë¥˜ë³„)\n")
for major, minors in sorted(major_to_minors.items()):
    report_lines.append(f"### {major} ({len(minors)}ê°œ)")
    # 2ì—´ë¡œ í‘œì‹œí•˜ê¸° ìœ„í•´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜ìœ¼ë¡œ ë‚˜ëˆ”
    mid_point = (len(minors) + 1) // 2
    sorted_minors = sorted(minors)
    col1 = sorted_minors[:mid_point]
    col2 = sorted_minors[mid_point:]
    
    # ê° ì—´ì˜ ê¸¸ì´ë¥¼ ë§ì¶¤
    while len(col1) < len(col2):
        col1.append("")

    for i in range(len(col2)):
        report_lines.append(f"- {col1[i]:<40} - {col2[i]}")
    report_lines.append("\n")

# 8. ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ì €ì¥
with open(output_md_path, "w", encoding="utf-8") as f:
    f.write("\n".join(report_lines))

print(f"âœ… ë¶„ì„ ë¦¬í¬íŠ¸ê°€ '{output_md_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")