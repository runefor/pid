from collections import defaultdict, Counter
import json
import random
from pathlib import Path
import os
import shutil
from typing import Dict, List, Tuple, Union, Optional
from enum import Enum
import numpy as np
import cv2
import matplotlib.pyplot as plt
from abc import ABC, abstractmethod


class SplitStrategy(Enum):
    """ë¶„í•  ì „ëµ ì—´ê±°í˜•"""
    DOMINANT_CATEGORY = "dominant_category"
    MULTI_LABEL = "multi_label"
    HYBRID = "hybrid"
    ITERATIVE = "iterative"
    ITERATIVE_BY_ANNOTATION = "iterative_by_annotation"
    RANDOM = "random"
    
    def list():
        return list(map(lambda c: c.value, SplitStrategy))


class StratifiedDatasetSplitter:
    """
    COCO í˜•ì‹ ë°ì´í„°ì…‹ì„ ìœ„í•œ ì¢…í•© Stratified Split í´ë˜ìŠ¤
    
    ì„¸ ê°€ì§€ ì „ëµ ì§€ì›:
    1. Dominant Category: ì´ë¯¸ì§€ë‹¹ ê°€ì¥ ë§ì€ ì–´ë…¸í…Œì´ì…˜ì„ ê°€ì§„ ì¹´í…Œê³ ë¦¬ ê¸°ì¤€
    2. Multi-label: ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì¡°í•©ì„ ê³ ë ¤í•œ ì •ë°€ ë¶„í• 
    3. Hybrid: Dominant + ì†Œìˆ˜ ì¹´í…Œê³ ë¦¬ ë³´ì •
    4. Iterative: ì „ì²´ ë¶„í¬ ê· í˜•ì„ ìµœì í™”í•˜ëŠ” ë°˜ë³µì  ë¶„í•  (ê°€ì¥ ê°•ë ¥)
    5. Iterative by Annotation: ì–´ë…¸í…Œì´ì…˜ ë‹¨ìœ„ë¡œ ë¶„í•  (ê°€ì¥ ì •ë°€, ì´ë¯¸ì§€ ë³µì œ ë°œìƒ)
    """
    
    def __init__(
        self,
        input_json_path: Union[str, Path],
        output_dir: Union[str, Path],
        image_dir: Optional[Union[str, Path]] = None,
        train_ratio: float = 0.7,
        val_ratio: float = 0.2,
        random_seed: int = 42,
        strategy: Union[str, SplitStrategy] = SplitStrategy.HYBRID
    ):
        """
        Args:
            input_json_path: ì…ë ¥ COCO JSON íŒŒì¼ ê²½ë¡œ
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            image_dir: ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ì´ë¯¸ì§€ ë³µì‚¬ìš©, ì„ íƒì‚¬í•­)
            train_ratio: í›ˆë ¨ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸: 0.7)
            val_ratio: ê²€ì¦ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸: 0.2)
            random_seed: ëœë¤ ì‹œë“œ
            strategy: ë¶„í•  ì „ëµ (dominant_category, multi_label, hybrid)
        """
        self.input_json_path = Path(input_json_path)
        self.output_dir = Path(output_dir)
        self.image_dir = Path(image_dir) if image_dir else None
        self.train_ratio = train_ratio
        self.val_ratio = val_ratio
        self.test_ratio = 1.0 - train_ratio - val_ratio
        self.random_seed = random_seed
        
        # ì „ëµ ì„¤ì •
        if isinstance(strategy, str):
            self.strategy = SplitStrategy(strategy)
        else:
            self.strategy = strategy
        
        # ì „ëµë³„ íŒŒë¼ë¯¸í„° (ì‚¬ìš©ìê°€ ìˆ˜ì • ê°€ëŠ¥)
        self.strategy_params = {
            'rare_threshold': 50,           # Hybridìš©: ì†Œìˆ˜ í´ë˜ìŠ¤ ì„ê³„ê°’
            'very_rare_threshold': 10,      # Hybridìš©: ê·¹ì†Œìˆ˜ í´ë˜ìŠ¤ ì„ê³„ê°’  
            'min_samples_per_category': 3,  # ì¹´í…Œê³ ë¦¬ë‹¹ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
            'min_samples_per_split': 2,     # Splitë‹¹ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
            'rebalance_tolerance': 0.1      # Hybridìš©: ì¬ì¡°ì • í—ˆìš© ì˜¤ì°¨
        }
        
        # ë‚´ë¶€ ë°ì´í„°
        self.data = None
        self.images = None
        self.annotations = None
        self.categories = None
        self.img_to_anns = defaultdict(list)
        self.valid_images = None
        self.class_stats = {}
        
        random.seed(self.random_seed)
    
    def set_strategy_params(self, **params):
        """ì „ëµë³„ íŒŒë¼ë¯¸í„° ì„¤ì •"""
        self.strategy_params.update(params)
        return self
    
    def load_data(self):
        """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        print(f"ğŸ“ Loading data from: {self.input_json_path}")
        
        with open(self.input_json_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
        
        self.images = self.data['images']
        self.annotations = self.data['annotations']
        self.categories = self.data['categories']
        
        print(f"ğŸ“Š Dataset Overview:")
        print(f"   Total images: {len(self.images):,}")
        print(f"   Total annotations: {len(self.annotations):,}")
        print(f"   Total categories: {len(self.categories):,}")
        
        # ì´ë¯¸ì§€ë³„ ì–´ë…¸í…Œì´ì…˜ ë§¤í•‘
        self.img_to_anns = defaultdict(list)
        for ann in self.annotations:
            self.img_to_anns[ann['image_id']].append(ann)
        
        # --- ID ì¼ì¹˜ ë¶„ì„ ì½”ë“œ ì¶”ê°€ ---
        all_image_ids = {img['id'] for img in self.images}
        ann_image_ids = set(self.img_to_anns.keys())
        matching_ids = all_image_ids.intersection(ann_image_ids)
        non_matching_ann_ids = ann_image_ids - all_image_ids

        print("\nğŸ” Analyzing image and annotation ID matching:")
        print(f"   - Found {len(all_image_ids)} unique image IDs in 'images' list.")
        print(f"   - Found {len(ann_image_ids)} unique image IDs in 'annotations' list.")
        print(f"   - Found {len(matching_ids)} matching image IDs between them.")

        if len(ann_image_ids) > 0 and len(matching_ids) == 0:
            print("   - âš ï¸ WARNING: No annotation 'image_id' matches any 'id' in the 'images' list.")
            if len(non_matching_ann_ids) > 0:
                print(f"   - Example non-matching annotation image_id(s): {list(non_matching_ann_ids)[:5]}")
        print()
        # --- ID ì¼ì¹˜ ë¶„ì„ ì½”ë“œ ë ---

        # ì–´ë…¸í…Œì´ì…˜ì´ ìˆëŠ” ì´ë¯¸ì§€ë§Œ í•„í„°ë§
        self.valid_images = [img for img in self.images if img['id'] in self.img_to_anns]
        print(f"   Valid images (with annotations): {len(self.valid_images):,}")
        
        # í´ë˜ìŠ¤ë³„ í†µê³„ ë¶„ì„
        self._analyze_class_statistics()
        
        return self
    
    def split(self) -> Dict[str, Dict[str, int]]:
        """ì„ íƒëœ ì „ëµìœ¼ë¡œ ë°ì´í„° ë¶„í•  ì‹¤í–‰"""
        if self.data is None:
            self.load_data()
        
        # --- ê·¹ì†Œìˆ˜ í´ë˜ìŠ¤ ì‚¬ì „ ë¶„í•  ë¡œì§ ---
        pre_splits = {"train": [], "val": [], "test": []}
        pre_assigned_image_ids = set()

        # ì´ë¯¸ì§€ê°€ 2ê°œì¸ í´ë˜ìŠ¤ ì°¾ê¸°
        for class_id, stats in self.class_stats.items():
            if stats['appears_in_images'] == 2:
                class_name = stats['name']
                print(f"   Applying pre-split for rare class '{class_name}' with 2 images.")
                
                # í•´ë‹¹ í´ë˜ìŠ¤ë¥¼ í¬í•¨í•˜ëŠ” ì´ë¯¸ì§€ 2ê°œ ì°¾ê¸°
                images_for_class = [
                    img for img in self.valid_images 
                    if any(ann['category_id'] == class_id for ann in self.img_to_anns[img['id']])
                ]
                
                # ì´ë¯¸ í• ë‹¹ëœ ì´ë¯¸ì§€ëŠ” ê±´ë„ˆë›°ê¸°
                images_to_assign = [img for img in images_for_class if img['id'] not in pre_assigned_image_ids]
                if len(images_to_assign) == 2:
                    random.shuffle(images_to_assign)
                    pre_splits["train"].append(images_to_assign[0])
                    pre_splits["val"].append(images_to_assign[1])
                    pre_assigned_image_ids.add(images_to_assign[0]['id'])
                    pre_assigned_image_ids.add(images_to_assign[1]['id'])

        # ì‚¬ì „ í• ë‹¹ëœ ì´ë¯¸ì§€ë¥¼ ì œì™¸í•œ ìœ íš¨ ì´ë¯¸ì§€ ëª©ë¡ ì—…ë°ì´íŠ¸
        original_valid_images = self.valid_images
        self.valid_images = [img for img in self.valid_images if img['id'] not in pre_assigned_image_ids]

        print(f"\nğŸ”„ Applying {self.strategy.value} strategy...")
        
        # ì „ëµë³„ ë¶„í•  ì‹¤í–‰
        if self.strategy == SplitStrategy.DOMINANT_CATEGORY:
            splits = self._dominant_category_split()
        elif self.strategy == SplitStrategy.MULTI_LABEL:
            splits = self._multi_label_split()
        elif self.strategy == SplitStrategy.HYBRID:
            splits = self._hybrid_split()
        elif self.strategy == SplitStrategy.ITERATIVE:
            splits = self._iterative_split()
        elif self.strategy == SplitStrategy.RANDOM:
            splits = self._random_split()
        elif self.strategy == SplitStrategy.ITERATIVE_BY_ANNOTATION:
            return self._iterative_split_by_annotation()
        else:
            raise ValueError(f"Unknown strategy: {self.strategy}")

        # --- ì‚¬ì „ ë¶„í• ëœ ê²°ê³¼ì™€ ë³‘í•© ---
        for split_name in splits:
            splits[split_name].extend(pre_splits[split_name])

        # valid_imagesë¥¼ ì›ìƒíƒœë¡œ ë³µêµ¬
        self.valid_images = original_valid_images

        # ê²°ê³¼ ì €ì¥ ë° ê²€ì¦
        stats, _ = self._save_and_validate_splits(splits)
        
        return stats

    
    def _analyze_class_statistics(self):
        """í´ë˜ìŠ¤ë³„ ìƒì„¸ í†µê³„ ë¶„ì„"""
        # ì „ì²´ í´ë˜ìŠ¤ë³„ ì–´ë…¸í…Œì´ì…˜ ìˆ˜
        total_class_counts = Counter()
        for img in self.valid_images:
            for ann in self.img_to_anns[img['id']]:
                total_class_counts[ann['category_id']] += 1
        
        # í´ë˜ìŠ¤ë³„ ì´ë¯¸ì§€ ìˆ˜ (í•´ë‹¹ í´ë˜ìŠ¤ê°€ dominantì¸ ì´ë¯¸ì§€)
        dominant_image_counts = Counter()
        for img in self.valid_images:
            class_counts = Counter()
            for ann in self.img_to_anns[img['id']]:
                class_counts[ann['category_id']] += 1
            
            if class_counts:
                dominant_class = class_counts.most_common(1)[0][0]
                dominant_image_counts[dominant_class] += 1
        
        # í†µê³„ ì •ë¦¬
        category_dict = {cat['id']: cat['name'] for cat in self.categories}
        
        for class_id in total_class_counts.keys():
            self.class_stats[class_id] = {
                'name': category_dict.get(class_id, f'Unknown_{class_id}'),
                'total_annotations': total_class_counts[class_id],
                'dominant_images': dominant_image_counts.get(class_id, 0),
                'appears_in_images': len([1 for img in self.valid_images 
                                        if any(ann['category_id'] == class_id 
                                              for ann in self.img_to_anns[img['id']])])
            }
        
        # ë¶ˆê· í˜• ì •ë„ ê³„ì‚°
        counts = list(total_class_counts.values())
        if counts:
            max_count = max(counts)
            min_count = min(counts)
            imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')
            
            print(f"ğŸ“ˆ Class Distribution Analysis:")
            print(f"   Most frequent class: {max_count:,} annotations")
            print(f"   Least frequent class: {min_count:,} annotations") 
            print(f"   Imbalance ratio: {imbalance_ratio:.1f}:1")
            
            # ë¶„í¬ ì¶”ì²œ
            if imbalance_ratio < 10:
                recommended = "dominant_category"
            elif imbalance_ratio < 100:
                recommended = "hybrid"
            else:
                recommended = "iterative_by_annotation" # ê°€ì¥ ê°•ë ¥í•œ iterative_by_annotation ì¶”ì²œ
            
            if self.strategy.value != recommended:
                print(f"ğŸ’¡ Recommended strategy for this dataset: {recommended}")
                print(f"   (Currently using: {self.strategy.value})")
    
    def _dominant_category_split(self) -> Dict[str, List[Dict]]:
        """Dominant Category ê¸°ë°˜ ë¶„í• """
        print("   Using dominant category per image...")
        
        # í´ë˜ìŠ¤ë³„ ì´ë¯¸ì§€ ê·¸ë£¹í™”
        class_to_images = defaultdict(list)
        
        for img in self.valid_images:
            class_counts = Counter()
            for ann in self.img_to_anns[img['id']]:
                class_counts[ann['category_id']] += 1
            
            if class_counts:
                # ë™ì ì‹œ í´ë˜ìŠ¤ IDê°€ ì‘ì€ ê²ƒì„ ìš°ì„  ì„ íƒ (ì¼ê´€ì„± ìœ„í•´)
                sorted_classes = sorted(class_counts.items(), 
                                      key=lambda x: (-x[1], x[0]))
                dominant_class = sorted_classes[0][0]
                class_to_images[dominant_class].append(img)
        
        return self._split_by_class_groups(class_to_images)
    
    def _multi_label_split(self) -> Dict[str, List[Dict]]:
        """Multi-label ê¸°ë°˜ ë¶„í•  (ì¹´í…Œê³ ë¦¬ ì¡°í•© ê³ ë ¤)"""
        print("   Using multi-label combinations...")
        
        # ì´ë¯¸ì§€ë³„ ì¹´í…Œê³ ë¦¬ ì¡°í•© ìƒì„±
        combination_to_images = defaultdict(list)
        
        for img in self.valid_images:
            categories_in_image = set()
            for ann in self.img_to_anns[img['id']]:
                categories_in_image.add(ann['category_id'])
            
            # frozensetìœ¼ë¡œ ì¡°í•© ìƒì„± (ìˆœì„œ ë¬´ê´€)
            combination = frozenset(categories_in_image)
            combination_to_images[combination].append(img)
        
        print(f"   Found {len(combination_to_images)} unique category combinations")
        
        # ì¡°í•©ë³„ ë¶„í• 
        splits = {"train": [], "val": [], "test": []}
        
        for combination, comb_images in combination_to_images.items():
            n_images = len(comb_images)
            
            if n_images < self.strategy_params['min_samples_per_category']:
                # ìƒ˜í”Œì´ ë„ˆë¬´ ì ìœ¼ë©´ trainì— ëª¨ë‘ í• ë‹¹
                splits["train"].extend(comb_images)
                continue
            
            # ë¹„ìœ¨ì— ë”°ë¼ ë¶„í• 
            random.shuffle(comb_images)
            
            n_train = max(1, int(n_images * self.train_ratio))
            n_val = max(1, int(n_images * self.val_ratio))
            
            splits["train"].extend(comb_images[:n_train])
            splits["val"].extend(comb_images[n_train:n_train + n_val])
            splits["test"].extend(comb_images[n_train + n_val:])
        
        return splits

    def _random_split(self) -> Dict[str, List[Dict]]:
        """ë‹¨ìˆœ ëœë¤ ë¶„í• """
        print("   Using simple random split...")
        
        # ì‚¬ì „ í• ë‹¹ëœ ì´ë¯¸ì§€ë¥¼ ì œì™¸í•œ ì´ë¯¸ì§€ ëª©ë¡ì„ ë³µì‚¬í•˜ì—¬ ì‚¬ìš©
        images_to_split = self.valid_images.copy()
        random.shuffle(images_to_split)
        
        n_images = len(images_to_split)
        n_train = int(n_images * self.train_ratio)
        n_val = int(n_images * self.val_ratio)
        
        splits = {
            "train": images_to_split[:n_train],
            "val": images_to_split[n_train:n_train + n_val],
            "test": images_to_split[n_train + n_val:]
        }
        
        return splits
    
    def _hybrid_split(self) -> Dict[str, List[Dict]]:
        """Hybrid ë°©ì‹: Dominant Category + ì†Œìˆ˜ í´ë˜ìŠ¤ ë³´ì •"""
        print("   Using hybrid approach (dominant + rare class rebalancing)...")
        
        # 1ë‹¨ê³„: í´ë˜ìŠ¤ë¥¼ í‹°ì–´ë³„ë¡œ ë¶„ë¥˜
        rare_threshold = self.strategy_params['rare_threshold']
        very_rare_threshold = self.strategy_params['very_rare_threshold']
        
        class_tiers = self._classify_class_tiers(rare_threshold, very_rare_threshold)
        
        # 2ë‹¨ê³„: ì´ë¯¸ì§€ë³„ dominant class ê²°ì • (ì†Œìˆ˜ í´ë˜ìŠ¤ ìš°ì„ ìˆœìœ„ ë¶€ì—¬)
        class_to_images = defaultdict(list)
        
        for img in self.valid_images:
            class_counts = Counter()
            for ann in self.img_to_anns[img['id']]:
                class_counts[ann['category_id']] += 1
            
            if class_counts:
                # ì†Œìˆ˜ í´ë˜ìŠ¤ê°€ ìˆìœ¼ë©´ ìš°ì„ ìˆœìœ„ ë¶€ì—¬
                rare_classes = [cls for cls in class_counts.keys() 
                              if cls in class_tiers['rare'] or cls in class_tiers['very_rare']]
                
                if rare_classes:
                    # ì†Œìˆ˜ í´ë˜ìŠ¤ ì¤‘ ê°€ì¥ ë§ì€ ê²ƒì„ ì„ íƒ
                    dominant_class = max(rare_classes, key=lambda x: class_counts[x])
                else:
                    # ì¼ë°˜ì ì¸ dominant class ì„ íƒ
                    dominant_class = class_counts.most_common(1)[0][0]
                
                class_to_images[dominant_class].append(img)
        
        # 3ë‹¨ê³„: í‹°ì–´ë³„ë¡œ ë‹¤ë¥¸ ë¶„í•  ì „ëµ ì ìš©
        return self._split_by_tiers(class_to_images, class_tiers)

    def _iterative_split(self) -> Dict[str, List[Dict]]:
        """
        ë°˜ë³µì  ê³„ì¸µí™” ë¶„í•  (Iterative Stratification)
        ì „ì²´ ì¹´í…Œê³ ë¦¬ ë¶„í¬ë¥¼ ìµœì í™”í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ í•˜ë‚˜ì”© í• ë‹¹í•©ë‹ˆë‹¤.
        """
        print("Using iterative stratification for optimal balance...")

        # 1. ì´ˆê¸° ì„¤ì •
        unassigned_images = self.valid_images.copy() # ì‚¬ì „ í• ë‹¹ëœ ì´ë¯¸ì§€ê°€ ì œì™¸ëœ ë¦¬ìŠ¤íŠ¸
        random.shuffle(unassigned_images)
        
        # ì´ë¯¸ì§€ ID -> ì´ë¯¸ì§€ ì •ë³´, ì¹´í…Œê³ ë¦¬ Set ë§¤í•‘
        img_id_to_info = {img['id']: img for img in unassigned_images}
        img_id_to_cats = {
            img_id: frozenset(ann['category_id'] for ann in anns)
            for img_id, anns in self.img_to_anns.items()
        }

        # 2. ëª©í‘œ ë¶„í¬ ê³„ì‚° (ì–´ë…¸í…Œì´ì…˜ ìˆ˜ ê¸°ì¤€)
        target_dist = {
            'train': self.train_ratio,
            'val': self.val_ratio,
            'test': self.test_ratio
        }
        target_ann_counts = defaultdict(lambda: defaultdict(float))
        for cat_id, stats in self.class_stats.items():
            total_anns = stats['total_annotations']
            for split in target_dist:
                target_ann_counts[cat_id][split] = total_anns * target_dist[split]

        # 3. í˜„ì¬ ë¶„í•  ìƒíƒœ ì´ˆê¸°í™”
        splits = {"train": [], "val": [], "test": []}
        current_ann_counts = defaultdict(lambda: Counter())
        
        # 4. ì´ë¯¸ì§€ ë°˜ë³µ í• ë‹¹
        pbar = range(len(unassigned_images))
        for _ in pbar:
            # ê°€ì¥ í¬ê·€í•œ ì¹´í…Œê³ ë¦¬ë¥¼ ê°€ì§„ ì´ë¯¸ì§€ë¥¼ ì°¾ìŒ
            # (ì „ì²´ ë°ì´í„°ì…‹ì—ì„œ ê°€ì¥ ì ê²Œ ë‚˜íƒ€ë‚˜ëŠ” ì¹´í…Œê³ ë¦¬)
            min_cat_count = float('inf')
            best_img_id = -1
            
            # ì•„ì§ í• ë‹¹ë˜ì§€ ì•Šì€ ì´ë¯¸ì§€ ì¤‘ì—ì„œ ì„ íƒ
            unassigned_ids = list(img_id_to_info.keys())
            if not unassigned_ids: break

            for img_id in unassigned_ids:
                cats_in_img = img_id_to_cats.get(img_id, set())
                if not cats_in_img: continue
                
                # ì´ë¯¸ì§€ì— í¬í•¨ëœ ì¹´í…Œê³ ë¦¬ ì¤‘ ê°€ì¥ í¬ê·€í•œ ì¹´í…Œê³ ë¦¬ì˜ ë“±ì¥ íšŸìˆ˜
                rarest_cat_count_in_img = min(self.class_stats[cat_id]['appears_in_images'] for cat_id in cats_in_img)
                
                if rarest_cat_count_in_img < min_cat_count:
                    min_cat_count = rarest_cat_count_in_img
                    best_img_id = img_id
            
            if best_img_id == -1: # ë‚¨ì€ ì´ë¯¸ì§€ê°€ ì–´ë…¸í…Œì´ì…˜ì´ ì—†ëŠ” ê²½ìš°
                best_img_id = unassigned_ids[0]

            # 5. ìµœì ì˜ split ì°¾ê¸°
            # ì´ë¯¸ì§€ë¥¼ ê° splitì— ì¶”ê°€í–ˆì„ ë•Œ ëª©í‘œ ë¶„í¬ì™€ì˜ ì°¨ì´ê°€ ê°€ì¥ ì ì€ ê³³ì„ ì„ íƒ
            best_split = ''
            min_diff = float('inf')
            
            img_cats = img_id_to_cats.get(best_img_id, set())
            
            for split_name in splits.keys():
                diff = 0
                # ì´ splitì— ì´ë¯¸ì§€ë¥¼ ì¶”ê°€í–ˆì„ ë•Œ, ê° ì¹´í…Œê³ ë¦¬ì˜ ëª©í‘œ ë‹¬ì„±ë¥ ì„ ê³„ì‚°
                for cat_id in img_cats:
                    # ì´ splitì— ìˆëŠ” í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ í˜„ì¬ ì–´ë…¸í…Œì´ì…˜ ìˆ˜
                    current_ann_count = current_ann_counts[split_name].get(cat_id, 0)
                    total_ann_for_cat = self.class_stats[cat_id]['total_annotations']
                    
                    # ì´ splitì˜ ëª©í‘œ ì–´ë…¸í…Œì´ì…˜ ìˆ˜
                    target_ratio = target_dist[split_name]
                    target_ann_count_for_split = total_ann_for_cat * target_ratio
                    
                    # ëª©í‘œ ëŒ€ë¹„ í˜„ì¬ ì–¼ë§ˆë‚˜ ì±„ì›Œì¡ŒëŠ”ì§€ ë¹„ìœ¨ì„ ê³„ì‚°
                    # ì´ ê°’ì´ ì‘ì„ìˆ˜ë¡ í•´ë‹¹ split/categoryì— ì´ë¯¸ì§€ê°€ ë” í•„ìš”í•˜ë‹¤ëŠ” ì˜ë¯¸
                    # +1ì„ í•˜ì—¬ 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë°©ì§€í•˜ê³ , ì•„ì§ í• ë‹¹ë˜ì§€ ì•Šì€ ê²½ìš°ë¥¼ ì²˜ë¦¬
                    fulfillment_ratio = (current_ann_count + 1) / (target_ann_count_for_split + 1)
                    diff += fulfillment_ratio
                
                if diff < min_diff:
                    min_diff = diff
                    best_split = split_name

            # 6. ì´ë¯¸ì§€ í• ë‹¹ ë° ìƒíƒœ ì—…ë°ì´íŠ¸
            splits[best_split].append(img_id_to_info[best_img_id])
            for cat_id in img_cats:
                current_ann_counts[best_split][cat_id] += 1
            del img_id_to_info[best_img_id]

        return splits
    
    def _iterative_split_by_annotation(self) -> Dict[str, Dict[str, int]]:
        """
        ì–´ë…¸í…Œì´ì…˜ ë‹¨ìœ„ì˜ ë°˜ë³µì  ê³„ì¸µí™” ë¶„í• .
        ì´ë¯¸ì§€ë¥¼ ë³µì œí•˜ì—¬ ê° splitì— í•„ìš”í•œ ì–´ë…¸í…Œì´ì…˜ë§Œ í¬í•¨ì‹œí‚µë‹ˆë‹¤.
        """
        print("Using iterative stratification by annotation (most precise)...")

        # 1. ëª©í‘œ ë¹„ìœ¨ ì„¤ì •
        target_dist = {
            'train': self.train_ratio,
            'val': self.val_ratio,
            'test': self.test_ratio
        }

        # 2. ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì–´ë…¸í…Œì´ì…˜ ë¶„í• 
        split_annotations = defaultdict(list)
        cat_to_anns = defaultdict(list)
        for ann in self.annotations:
            cat_to_anns[ann['category_id']].append(ann)

        for cat_id, anns in cat_to_anns.items():
            random.shuffle(anns)
            n_anns = len(anns)
            n_train = int(n_anns * self.train_ratio)
            n_val = int(n_anns * self.val_ratio)
            
            split_annotations['train'].extend(anns[:n_train])
            split_annotations['val'].extend(anns[n_train:n_train + n_val])
            split_annotations['test'].extend(anns[n_train + n_val:])

        # 3. ë¶„í• ëœ ì–´ë…¸í…Œì´ì…˜ì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ë°ì´í„° êµ¬ì¡° ìƒì„± ë° ì €ì¥
        self.output_dir.mkdir(parents=True, exist_ok=True)
        (self.output_dir / "annotations").mkdir(parents=True, exist_ok=True)
        
        stats = {}
        img_id_map = {img['id']: img for img in self.images}

        for split_name, anns in split_annotations.items():
            if not anns:
                continue

            # ì´ splitì— í¬í•¨ëœ ì´ë¯¸ì§€ ID ìˆ˜ì§‘
            image_ids_in_split = {ann['image_id'] for ann in anns}
            split_images = [img_id_map[img_id] for img_id in image_ids_in_split if img_id in img_id_map]

            # ID ì¬í• ë‹¹
            split_images_copy = [img.copy() for img in split_images]
            split_annotations_copy = [ann.copy() for ann in anns]
            self._reassign_ids(split_images_copy, split_annotations_copy)

            # JSON ì €ì¥
            split_data = {
                'info': self.data.get('info', {}),
                'licenses': self.data.get('licenses', []),
                'images': split_images_copy,
                'annotations': split_annotations_copy,
                'categories': self.categories
            }
            json_path = self.output_dir / "annotations" / f'{split_name}.json'
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(split_data, f, ensure_ascii=False, indent=2)

            # ì´ë¯¸ì§€ ë³µì‚¬ (ì˜µì…˜)
            if self.image_dir:
                self._copy_images(split_images, self.output_dir / split_name)

            # í†µê³„ ìˆ˜ì§‘
            stats[split_name] = {
                'images': len(split_images_copy),
                'annotations': len(split_annotations_copy),
                'categories': dict(Counter(ann['category_id'] for ann in split_annotations_copy))
            }
            print(f"   {split_name}: {len(split_images_copy):,} images, {len(split_annotations_copy):,} annotations")

        # ê²€ì¦ ë¦¬í¬íŠ¸ ìƒì„±
        self._generate_validation_report(stats, self.output_dir / 'validation_report.md', {k: {ann['image_id'] for ann in v} for k, v in split_annotations.items()})
        # í†µê³„ì™€ ì´ë¯¸ì§€ ID ì…‹ì„ í•¨ê»˜ ë°˜í™˜
        return stats, {k: {ann['image_id'] for ann in v} for k, v in split_annotations.items()}

    def _classify_class_tiers(self, rare_threshold, very_rare_threshold):
        """í´ë˜ìŠ¤ë¥¼ í‹°ì–´ë³„ë¡œ ë¶„ë¥˜"""
        tiers = {
            'major': [],
            'medium': [],
            'rare': [],
            'very_rare': []
        }
        
        for class_id, stats in self.class_stats.items():
            count = stats['total_annotations']
            
            if count < very_rare_threshold:
                tiers['very_rare'].append(class_id)
            elif count < rare_threshold:
                tiers['rare'].append(class_id)
            elif count < 1000:
                tiers['medium'].append(class_id)
            else:
                tiers['major'].append(class_id)
        
        print(f"   Class tiers: Major({len(tiers['major'])}), "
              f"Medium({len(tiers['medium'])}), "
              f"Rare({len(tiers['rare'])}), "
              f"VeryRare({len(tiers['very_rare'])})")
        
        return tiers
    
    def _split_by_class_groups(self, class_to_images):
        """í´ë˜ìŠ¤ë³„ ì´ë¯¸ì§€ ê·¸ë£¹ì„ ë¶„í• """
        splits = {"train": [], "val": [], "test": []}
        
        for class_id, images in class_to_images.items():
            if len(images) < self.strategy_params['min_samples_per_category']:
                splits["train"].extend(images)
                continue
            
            random.shuffle(images)
            
            n_train = max(1, int(len(images) * self.train_ratio))
            n_val = max(1, int(len(images) * self.val_ratio))
            
            splits["train"].extend(images[:n_train])
            splits["val"].extend(images[n_train:n_train + n_val])
            splits["test"].extend(images[n_train + n_val:])
        
        return splits
    
    def _split_by_tiers(self, class_to_images, class_tiers):
        """í‹°ì–´ë³„ë¡œ ë‹¤ë¥¸ ì „ëµìœ¼ë¡œ ë¶„í• """
        splits = {"train": [], "val": [], "test": []}
        
        for tier_name, class_ids in class_tiers.items():
            for class_id in class_ids:
                images = class_to_images.get(class_id, [])
                if not images:
                    continue
                
                n_images = len(images)
                random.shuffle(images)
                
                if tier_name == 'very_rare':
                    if n_images >= self.strategy_params['min_samples_per_category']: # ê¸°ë³¸ê°’ 3 ì´ìƒ
                        # 3ê°œì¼ ê²½ìš°: train 1, val 1, test 1
                        # 4ê°œì¼ ê²½ìš°: train 2, val 1, test 1
                        # 5ê°œì¼ ê²½ìš°: train 3, val 1, test 1
                        splits["test"].extend(images[:1])
                        splits["val"].extend(images[1:2])
                        splits["train"].extend(images[2:])
                    else:
                        # ìƒ˜í”Œ ìˆ˜ê°€ ë¶€ì¡±í•˜ë©´ ëª¨ë‘ trainì— í• ë‹¹
                        splits["train"].extend(images)
                        
                elif tier_name == 'rare':
                    # ì†Œìˆ˜: ìµœì†Œ ìƒ˜í”Œ ìˆ˜ë¥¼ ë§Œì¡±í•˜ë©´ ë³´ìˆ˜ì  ë¶„í• , ì•„ë‹ˆë©´ ëª¨ë‘ train
                    if n_images >= self.strategy_params['min_samples_per_category']: # ê¸°ë³¸ê°’ 3 ì´ìƒ
                        # ë³´ìˆ˜ì  ë¶„í•  (ì˜ˆ: 80:10:10) ì‹œë„, ê° splitì— ìµœì†Œ 1ê°œ ë³´ì¥
                        n_val = max(1, int(n_images * 0.1))
                        n_test = max(1, int(n_images * 0.1))
                        n_train = n_images - n_val - n_test
                        
                        splits["test"].extend(images[:n_test])
                        splits["val"].extend(images[n_test:n_test + n_val])
                        splits["train"].extend(images[n_test + n_val:])
                    else:
                        splits["train"].extend(images)
                        
                else:
                    # ì¼ë°˜: í‘œì¤€ ë¹„ìœ¨ ë¶„í• 
                    n_train = int(n_images * self.train_ratio)
                    n_val = int(n_images * self.val_ratio)
                    
                    splits["train"].extend(images[:n_train])
                    splits["val"].extend(images[n_train:n_train + n_val])
                    splits["test"].extend(images[n_train + n_val:])
        
        return splits
    
    def _save_and_validate_splits(self, splits):
        """ë¶„í•  ê²°ê³¼ ì €ì¥ ë° ê²€ì¦"""
        self.output_dir.mkdir(parents=True, exist_ok=True)
        (self.output_dir / "annotations").mkdir(parents=True, exist_ok=True)
        
        stats = {}
        all_split_image_ids = defaultdict(set)
        
        for split_name, split_images in splits.items():
            if not split_images:
                continue
            
            # í•´ë‹¹ splitì˜ ì–´ë…¸í…Œì´ì…˜ ìˆ˜ì§‘
            split_annotations = []
            for img in split_images:
                split_annotations.extend(self.img_to_anns[img['id']])
                all_split_image_ids[split_name].add(img['id'])
            
            # ID ì¬í• ë‹¹
            split_images_copy = [img.copy() for img in split_images]
            split_annotations_copy = [ann.copy() for ann in split_annotations]
            
            self._reassign_ids(split_images_copy, split_annotations_copy)
            
            # JSON ì €ì¥
            split_data = {
                'info': self.data.get('info', {}),
                'licenses': self.data.get('licenses', []),
                'images': split_images_copy,
                'annotations': split_annotations_copy,
                'categories': self.categories
            }
            
            json_path = self.output_dir / "annotations" / f'{split_name}.json'
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(split_data, f, ensure_ascii=False, indent=2)
            
            # ì´ë¯¸ì§€ ë³µì‚¬ (ì˜µì…˜)
            if self.image_dir:
                self._copy_images(split_images, self.output_dir / split_name)
            
            # í†µê³„ ìˆ˜ì§‘
            class_counts = Counter()
            for ann in split_annotations_copy:
                class_counts[ann['category_id']] += 1
            
            stats[split_name] = {
                'images': len(split_images_copy),
                'annotations': len(split_annotations_copy),
                'categories': dict(class_counts)
            }
            
            print(f"   {split_name}: {len(split_images_copy):,} images, "
                  f"{len(split_annotations_copy):,} annotations")
        
        # ê²€ì¦ ë¦¬í¬íŠ¸ ìƒì„±
        report_path = self.output_dir / 'validation_report.md'
        self._generate_validation_report(stats, report_path, all_split_image_ids)
        
        print(f"\nâœ… Split completed! Results saved to: {self.output_dir}")
        print(f"   ğŸ“Š Validation report saved to: {report_path}")
        return stats, all_split_image_ids
    
    def _reassign_ids(self, images, annotations):
        """ID ì¬í• ë‹¹"""
        old_to_new_image_id = {}
        for i, img in enumerate(images, 1):
            old_id = img['id']
            img['id'] = i
            old_to_new_image_id[old_id] = i
        
        for i, ann in enumerate(annotations, 1):
            ann['id'] = i
            ann['image_id'] = old_to_new_image_id[ann['image_id']]
    
    def _copy_images(self, images, dst_dir):
        """ì´ë¯¸ì§€ ë³µì‚¬"""
        if not self.image_dir or not self.image_dir.exists():
            print(f"   Warning: Image directory not found: {self.image_dir}")
            return
        
        dst_dir.mkdir(exist_ok=True)
        
        copied = 0
        for img in images:
            src_path = self.image_dir / img['file_name']
            dst_path = dst_dir / img['file_name']
            
            if src_path.exists():
                shutil.copy2(src_path, dst_path)
                copied += 1
        
        print(f"   Copied {copied}/{len(images)} images")
    
    def _generate_validation_report(self, stats: Dict[str, Dict], report_path: Path, all_split_image_ids: Dict[str, set]):
        """ê²€ì¦ ë¦¬í¬íŠ¸ë¥¼ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ìƒì„±"""
        report_lines = []

        report_lines.append(f"# ğŸ“‹ Stratified Split Validation Report ({self.strategy.value.upper()})")
        report_lines.append("\n")

        category_dict = {cat['id']: cat['name'] for cat in self.categories}

        # ì „ì²´ í†µê³„
        total_images = sum(split_stats['images'] for split_stats in stats.values())
        unique_image_ids = set.union(*all_split_image_ids.values()) if all_split_image_ids else set()
        unique_images_count = len(unique_image_ids)
        total_annotations = sum(split_stats['annotations'] for split_stats in stats.values())
        duplication_rate = (total_images / unique_images_count - 1) * 100 if unique_images_count > 0 else 0


        if total_images == 0:
            report_lines.append("No data to report.")
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write("\n".join(report_lines))
            return

        # --- ë¶„í•  ë¹„ìœ¨ ì •ë³´ ì¶”ê°€ ---
        train_imgs = stats.get('train', {}).get('images', 0)
        val_imgs = stats.get('val', {}).get('images', 0)
        test_imgs = stats.get('test', {}).get('images', 0)
        
        # ë¶„í• ëœ ì´ ì´ë¯¸ì§€ ìˆ˜ ê¸°ì¤€ ì‹¤ì œ ë¹„ìœ¨ ê³„ì‚° (ì¤‘ë³µ í¬í•¨)
        actual_train_ratio = train_imgs / total_images * 100 if total_images > 0 else 0
        actual_val_ratio = val_imgs / total_images * 100 if total_images > 0 else 0
        actual_test_ratio = test_imgs / total_images * 100 if total_images > 0 else 0

        report_lines.append("## ğŸ¯ Split Ratio Summary\n")
        report_lines.append(f"- **Target Ratio (Train:Val:Test)**: {self.train_ratio * 100:.0f} : {self.val_ratio * 100:.0f} : {self.test_ratio * 100:.0f}")
        report_lines.append(f"- **Actual Ratio (based on image counts per split)**: {actual_train_ratio:.1f} : {actual_val_ratio:.1f} : {actual_test_ratio:.1f}\n")
        # --- ë¶„í•  ë¹„ìœ¨ ì •ë³´ ì¶”ê°€ ë ---

        report_lines.append("## Overall Distribution\n")
        report_lines.append(f"- **Total Categories**: {len(self.categories):,}\n")
        report_lines.append(f"- **Unique Images**: {unique_images_count:,}")
        if duplication_rate > 0.1:
            report_lines.append(f"- **Image Duplication Rate**: {duplication_rate:.1f}% (An image can appear in multiple splits)\n")
        
        report_lines.append("| Split | Images | Image % | Annotations | Annotation % |")
        report_lines.append("|:------|-------:|--------:|------------:|-------------:|")
        for split_name, split_stats in stats.items():
            # ì´ë¯¸ì§€ %ëŠ” ë¶„í• ëœ ì´ ì´ë¯¸ì§€ ìˆ˜ ëŒ€ë¹„ë¡œ ê³„ì‚°
            img_pct = split_stats['images'] / total_images * 100 if total_images > 0 else 0
            ann_pct = split_stats['annotations'] / total_annotations * 100 if total_annotations > 0 else 0
            report_lines.append(f"| {split_name} | {split_stats['images']:,} | {img_pct:.1f}% | {split_stats['annotations']:,} | {ann_pct:.1f}% |")
        report_lines.append("\n")

        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬ (ìƒìœ„ 20ê°œë§Œ)
        report_lines.append("## Category Distribution (All Categories by Total Annotations)\n")
        report_lines.append("| Category | Train (Count) | Train (%) | Val (Count) | Val (%) | Test (Count) | Test (%) | Total |")
        report_lines.append("|:---|---:|---:|---:|---:|---:|---:|---:|")

        # ì „ì²´ ì¹´í…Œê³ ë¦¬ë¥¼ ì–´ë…¸í…Œì´ì…˜ ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        all_categories = set()
        for split_stats in stats.values():
            all_categories.update(split_stats['categories'].keys())

        category_totals = defaultdict(int)
        for cat_id in all_categories: # ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ ì „ì²´ ì–´ë…¸í…Œì´ì…˜ ìˆ˜ ê³„ì‚°
            total = sum(stats.get(split, {}).get('categories', {}).get(cat_id, 0)
                        for split in ['train', 'val', 'test'])
            category_totals[cat_id] = total

        # ìƒìœ„ 20ê°œ ì¹´í…Œê³ ë¦¬ ì¶œë ¥
        sorted_categories = sorted(category_totals.items(), key=lambda x: x[1], reverse=True)

        for cat_id, total in sorted_categories:
            cat_name = category_dict.get(cat_id, f'Unknown_{cat_id}')
            train_count = stats.get('train', {}).get('categories', {}).get(cat_id, 0)
            val_count = stats.get('val', {}).get('categories', {}).get(cat_id, 0)
            test_count = stats.get('test', {}).get('categories', {}).get(cat_id, 0)

            train_pct = (train_count / total * 100) if total > 0 else 0
            val_pct = (val_count / total * 100) if total > 0 else 0
            test_pct = (test_count / total * 100) if total > 0 else 0

            report_lines.append(f"| {cat_name} | {train_count:,} | {train_pct:.1f}% | {val_count:,} | {val_pct:.1f}% | {test_count:,} | {test_pct:.1f}% | {total:,} |")

        # ë¶„í¬ í’ˆì§ˆ ë¶„ì„
        self._add_distribution_quality_analysis(report_lines, stats, category_totals, category_dict)

        # ë¬¸ì œ ì¹´í…Œê³ ë¦¬ ë¶„ì„ (Val/Testì— ìƒ˜í”Œì´ ê±°ì˜ ì—†ëŠ” ê²½ìš°)
        problem_categories = []
        # min_samples_thresholdë¥¼ 2ë¡œ ì„¤ì •í•˜ì—¬ 0, 1, 2ê°œì¸ ê²½ìš°ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        min_samples_threshold = 2
        for cat_id in all_categories:
            val_count = stats.get('val', {}).get('categories', {}).get(cat_id, 0)
            test_count = stats.get('test', {}).get('categories', {}).get(cat_id, 0)

            if val_count <= min_samples_threshold or test_count <= min_samples_threshold:
                train_count = stats.get('train', {}).get('categories', {}).get(cat_id, 0)
                total = category_totals[cat_id]
                problem_categories.append({
                    'id': cat_id,
                    'name': category_dict.get(cat_id, f'Unknown_{cat_id}'),
                    'train': train_count,
                    'val': val_count,
                    'test': test_count,
                    'total': total
                })
        
        if problem_categories:
            report_lines.append("\n## âš ï¸ Problem Category Analysis (Low Samples in Val/Test)\n")
            report_lines.append(f"Categories with **{min_samples_threshold} or fewer** samples in validation or test splits.\n")
            report_lines.append("| Category | Train | Val | Test | Total |")
            report_lines.append("|:---|---:|---:|---:|---:|")
            # Val ìƒ˜í”Œ ìˆ˜, Test ìƒ˜í”Œ ìˆ˜, ì „ì²´ ìƒ˜í”Œ ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            sorted_problem_cats = sorted(problem_categories, key=lambda x: (x['val'], x['test'], x['total']))
            for cat in sorted_problem_cats:
                report_lines.append(f"| {cat['name']} | {cat['train']:,} | **{cat['val']:,}** | **{cat['test']:,}** | {cat['total']:,} |")

        # íŒŒì¼ ì €ì¥
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("\n".join(report_lines))

    def _add_distribution_quality_analysis(self, report_lines: list, stats: dict, category_totals: dict, category_dict: dict):
        """ë¶„í¬ í’ˆì§ˆ ë¶„ì„ ì„¹ì…˜ì„ ë¦¬í¬íŠ¸ì— ì¶”ê°€"""
        target_ratios = {
            'train': self.train_ratio,
            'val': self.val_ratio,
            'test': self.test_ratio
        }

        category_divergence = {}
        for cat_id, total in category_totals.items():
            if total == 0:
                continue
            
            divergence = 0
            for split_name, target_ratio in target_ratios.items():
                actual_count = stats.get(split_name, {}).get('categories', {}).get(cat_id, 0)
                actual_ratio = actual_count / total
                # ëª©í‘œ ë¹„ìœ¨ê³¼ì˜ ì ˆëŒ€ì ì¸ ì°¨ì´ë¥¼ í•©ì‚°
                divergence += abs(actual_ratio - target_ratio)
            
            category_divergence[cat_id] = divergence

        # í¸ì°¨ ì ìˆ˜ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_divergence = sorted(category_divergence.items(), key=lambda x: x[1], reverse=True)

        report_lines.append("\n## âš ï¸ Distribution Quality Analysis (Top 10 Most Skewed Categories)\n")
        report_lines.append("This section highlights categories whose distribution significantly deviates from the target ratio (e.g., 70:20:10).")
        report_lines.append("**Divergence Score**: A measure of how far the actual distribution is from the target. Higher is worse. (Max: 2.0)\n")
        report_lines.append("| Category | Train % | Val % | Test % | Divergence Score |")
        report_lines.append("|:---|---:|---:|---:|---:|")

        for cat_id, divergence_score in sorted_divergence[:10]:
            total = category_totals[cat_id]
            cat_name = category_dict.get(cat_id, f'Unknown_{cat_id}')
            train_pct = (stats.get('train', {}).get('categories', {}).get(cat_id, 0) / total * 100) if total > 0 else 0
            val_pct = (stats.get('val', {}).get('categories', {}).get(cat_id, 0) / total * 100) if total > 0 else 0
            test_pct = (stats.get('test', {}).get('categories', {}).get(cat_id, 0) / total * 100) if total > 0 else 0

            report_lines.append(f"| {cat_name} | {train_pct:.1f}% | {val_pct:.1f}% | {test_pct:.1f}% | **{divergence_score:.3f}** |")


def analyze_rare_class_locality(
    splitter: StratifiedDatasetSplitter,
    output_dir: Union[str, Path],
    source_dirs: List[Union[str, Path]],
    num_examples_per_class: int = 3
):
    """
    ì†Œìˆ˜ ì¹´í…Œê³ ë¦¬ì˜ ì§€ì—­ì„±(locality)ì„ ë¶„ì„í•˜ê³  ì‹œê°í™”í•©ë‹ˆë‹¤.
    ê° ì†Œìˆ˜/ê·¹ì†Œìˆ˜ ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´, í•´ë‹¹ ì–´ë…¸í…Œì´ì…˜ì´ í¬í•¨ëœ ì´ë¯¸ì§€ë¥¼ ì°¾ì•„
    ê·¸ ë¶„í¬ë¥¼ ì‹œê°í™”í•œ ì´ë¯¸ì§€ íŒŒì¼ì„ ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        splitter: ë°ì´í„°ê°€ ë¡œë“œëœ StratifiedDatasetSplitter ì¸ìŠ¤í„´ìŠ¤.
        output_dir: ì‹œê°í™” ê²°ê³¼ë¬¼ì„ ì €ì¥í•  ë””ë ‰í† ë¦¬.
        source_dirs: ì›ë³¸ ì´ë¯¸ì§€ íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ëª©ë¡.
        num_examples_per_class: í´ë˜ìŠ¤ë‹¹ ì‹œê°í™”í•  ìµœëŒ€ ì´ë¯¸ì§€ ì˜ˆì‹œ ìˆ˜.
    """
    if splitter.data is None:
        splitter.load_data()

    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"\nğŸ”¬ Analyzing locality of rare classes. Visualizations will be saved to: {output_dir}")

    # 1. ì†Œìˆ˜/ê·¹ì†Œìˆ˜ ì¹´í…Œê³ ë¦¬ ì‹ë³„
    rare_threshold = splitter.strategy_params['rare_threshold']
    very_rare_threshold = splitter.strategy_params['very_rare_threshold']
    class_tiers = splitter._classify_class_tiers(rare_threshold, very_rare_threshold)
    rare_class_ids = class_tiers['rare'] + class_tiers['very_rare']

    if not rare_class_ids:
        print("   No rare classes found to analyze.")
        return

    category_dict = {cat['id']: cat['name'] for cat in splitter.categories}
    image_map = {img['id']: img for img in splitter.images}

    # ì›ë³¸ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ë¹ ë¥´ê²Œ ì°¾ê¸° ìœ„í•œ ë§¤í•‘ ìƒì„±
    print("   Scanning source image directories...")
    image_path_map = {}
    for s_dir in source_dirs:
        s_dir = Path(s_dir)
        if not s_dir.exists(): continue
        for img_path in s_dir.rglob('*.png'):
            image_path_map[img_path.name] = img_path
    print(f"   Found {len(image_path_map)} unique source images.")

    # 2. ê° ì†Œìˆ˜ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì´ë¯¸ì§€ ë‚´ ë¶„í¬ ì‹œê°í™”
    for class_id in rare_class_ids:
        class_name_safe = category_dict.get(class_id, f"Unknown_{class_id}").replace("/", "_").replace("@", "_")
        print(f"   Analyzing class: {class_name_safe} (ID: {class_id})")

        # í•´ë‹¹ í´ë˜ìŠ¤ë¥¼ í¬í•¨í•˜ëŠ” ì´ë¯¸ì§€ì™€ ì–´ë…¸í…Œì´ì…˜ ì •ë³´ ìˆ˜ì§‘
        images_with_class = []
        for img_id, anns in splitter.img_to_anns.items():
            class_anns = [ann for ann in anns if ann['category_id'] == class_id]
            if class_anns:
                images_with_class.append({
                    'image_info': image_map[img_id],
                    'annotations': class_anns,
                    'count': len(class_anns)
                })

        if not images_with_class:
            continue

        # ì–´ë…¸í…Œì´ì…˜ ê°œìˆ˜ê°€ ë§ì€ ìˆœìœ¼ë¡œ ì •ë ¬
        images_with_class.sort(key=lambda x: x['count'], reverse=True)

        # ìƒìœ„ ì˜ˆì‹œ ì´ë¯¸ì§€ ì‹œê°í™”
        for i, item in enumerate(images_with_class[:num_examples_per_class]):
            img_info = item['image_info']
            # ì´ë¯¸ì§€ íŒŒì¼ëª…ì—ì„œ ê²½ë¡œë¥¼ ì œê±°í•˜ê³  ì•ˆì „í•œ íŒŒì¼ëª…ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
            base_filename = Path(img_info['file_name']).name
            save_path = output_dir / f"{class_name_safe}_example_{i+1}_{base_filename}.png"
            
            # ì›ë³¸ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ í•¨ê»˜ ì „ë‹¬
            source_image_path = image_path_map.get(base_filename)
            _visualize_annotations_on_image(img_info, item['annotations'], class_name_safe, save_path, source_image_path)

# í¸ì˜ í•¨ìˆ˜ë“¤
def create_splitter(
    input_json: Union[str, Path],
    output_dir: Union[str, Path],
    strategy: str = "hybrid",
    **kwargs
) -> StratifiedDatasetSplitter:
    """ê°„í¸í•œ splitter ìƒì„± í•¨ìˆ˜"""
    return StratifiedDatasetSplitter(
        input_json_path=input_json,
        output_dir=output_dir,
        strategy=strategy,
        **kwargs
    )


def quick_split(
    input_json: Union[str, Path],
    output_dir: Union[str, Path],
    strategy: str = "hybrid",
    train_ratio: float = 0.7,
    val_ratio: float = 0.2,
    **kwargs
) -> Tuple[Dict[str, Dict[str, int]], Dict[str, set]]:
    """ì›ìƒ· ë¶„í•  í•¨ìˆ˜"""
    splitter = create_splitter(
        input_json=input_json,
        output_dir=output_dir,
        strategy=strategy,
        train_ratio=train_ratio,
        val_ratio=val_ratio,
        **kwargs
    )
    
    # splitter.split()ì€ ì´ì œ (stats, image_ids) íŠœí”Œì„ ë°˜í™˜í•  ìˆ˜ ìˆìŒ
    result = splitter.split()
    if isinstance(result, tuple) and len(result) == 2:
        stats, image_ids = result
    else: # ì´ì „ ë²„ì „ í˜¸í™˜ì„±
        stats = result
        image_ids = {} # ID ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° ë¹ˆ ë”•ì…”ë„ˆë¦¬

    return stats, image_ids



# ì¶”ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def compare_strategies(
    input_json: Union[str, Path],
    output_base_dir: Union[str, Path] = "strategy_comparison",
    strategies: List[str] = None
) -> Dict[str, Dict[str, Dict[str, int]]]:
    """ì—¬ëŸ¬ ì „ëµì„ ë¹„êµí•˜ëŠ” í•¨ìˆ˜"""
    if strategies is None:
        strategies = SplitStrategy.list()
    
    results = {}
    all_image_ids_by_strategy = {}
    output_base = Path(output_base_dir)
    output_base.mkdir(parents=True, exist_ok=True)
    
    for strategy in strategies:
        print(f"\nğŸ”„ Comparing strategy: {strategy}")
        try:
            stats, image_ids = quick_split(
                input_json=input_json,
                output_dir=output_base / strategy,
                strategy=strategy
            )
            results[strategy] = stats
            all_image_ids_by_strategy[strategy] = image_ids
            
        except Exception as e:
            print(f"âŒ Error with {strategy}: {e}")
            results[strategy] = None
            all_image_ids_by_strategy[strategy] = None
    
    # ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±
    report_path = output_base / "comparison_report.md"
    _generate_comparison_report(results, report_path, all_image_ids_by_strategy)
    print(f"\nğŸ“Š Strategy comparison report saved to: {report_path}")
    return results


def _generate_comparison_report(results: Dict, output_path: Path, all_image_ids: Dict[str, Dict[str, set]]):
    """ì „ëµ ë¹„êµ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ê³  ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ì €ì¥"""
    report_lines = []
    console_lines = []

    # --- ì½˜ì†”ìš© í—¤ë” ---
    console_lines.append(f"\n" + "="*90)
    console_lines.append("ğŸ“Š STRATEGY COMPARISON REPORT")
    console_lines.append("="*90)
    console_lines.append(f"{'Strategy':<25} {'Train Imgs':<12} {'Val Imgs':<12} {'Test Imgs':<12} {'Unique Imgs':<13} {'Duplication':<12}")
    console_lines.append("-" * 90)

    # --- ë§ˆí¬ë‹¤ìš´ìš© í—¤ë” ---
    report_lines.append("# ğŸ“Š Strategy Comparison Report")
    report_lines.append("")
    report_lines.append("| Strategy | Train Images | Val Images | Test Images | Unique Images | Duplication |")
    report_lines.append("|:---|---:|---:|---:|---:|---:|")
    
    for strategy, stats in results.items():
        if stats is None:
            console_lines.append(f"{strategy:<25} {'ERROR':<12} {'ERROR':<12} {'ERROR':<12} {'ERROR':<15}")
            report_lines.append(f"| {strategy} | ERROR | ERROR | ERROR | ERROR |")
            continue
            
        train_imgs = stats.get('train', {}).get('images', 0)
        val_imgs = stats.get('val', {}).get('images', 0) 
        test_imgs = stats.get('test', {}).get('images', 0)
        
        image_ids = all_image_ids.get(strategy, {})
        unique_image_ids = set.union(*image_ids.values()) if image_ids else set()
        unique_imgs_count = len(unique_image_ids)
        total_imgs = train_imgs + val_imgs + test_imgs
        duplication_rate = (total_imgs / unique_imgs_count - 1) * 100 if unique_imgs_count > 0 else 0
        duplication_str = f"{duplication_rate:.1f}%"

        console_lines.append(f"{strategy:<25} {train_imgs:<12,} {val_imgs:<12,} {test_imgs:<12,} {unique_imgs_count:<13,} {duplication_str:<12}")
        report_lines.append(f"| {strategy} | {train_imgs:,} | {val_imgs:,} | {test_imgs:,} | {unique_imgs_count:,} | {duplication_str} |")

    # ì½˜ì†”ì— ì¶œë ¥
    print("\n".join(console_lines))

    # íŒŒì¼ì— ì €ì¥
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("\n".join(report_lines))


def analyze_dataset_characteristics(input_json: Union[str, Path]) -> Dict:
    """ë°ì´í„°ì…‹ íŠ¹ì„± ë¶„ì„í•˜ì—¬ ìµœì  ì „ëµ ì¶”ì²œ"""
    print("ğŸ” Analyzing dataset characteristics...")
    
    with open(input_json, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    images = data['images']
    annotations = data['annotations']
    categories = data['categories']
    
    # ê¸°ë³¸ í†µê³„
    img_to_anns = defaultdict(list)
    for ann in annotations:
        img_to_anns[ann['image_id']].append(ann)
    
    valid_images = [img for img in images if img['id'] in img_to_anns]
    
    # í´ë˜ìŠ¤ë³„ ë¶„í¬
    class_counts = Counter()
    for img in valid_images:
        for ann in img_to_anns[img['id']]:
            class_counts[ann['category_id']] += 1
    
    # ì´ë¯¸ì§€ë‹¹ í‰ê·  ì¹´í…Œê³ ë¦¬ ìˆ˜
    avg_categories_per_image = sum(len(anns) for anns in img_to_anns.values()) / len(valid_images)
    
    # ë¶ˆê· í˜• ì •ë„
    counts = list(class_counts.values())
    max_count = max(counts) if counts else 0
    min_count = min(counts) if counts else 0
    imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')
    
    # ì¹´í…Œê³ ë¦¬ ì¡°í•© ë³µì¡ë„
    combinations = set()
    for img in valid_images:
        categories_in_img = frozenset(ann['category_id'] for ann in img_to_anns[img['id']])
        combinations.add(categories_in_img)
    
    characteristics = {
        'total_images': len(valid_images),
        'total_categories': len(categories),
        'imbalance_ratio': imbalance_ratio,
        'avg_categories_per_image': avg_categories_per_image,
        'unique_combinations': len(combinations),
        'class_distribution': dict(class_counts)
    }
    
    # ì „ëµ ì¶”ì²œ
    if imbalance_ratio < 10:
        recommended_strategy = "dominant_category"
        reason = "Low imbalance, simple approach sufficient"
    elif imbalance_ratio < 100:
        recommended_strategy = "hybrid" 
        reason = "Moderate imbalance, hybrid approach optimal"
    else:
        recommended_strategy = "iterative_by_annotation"
        reason = "High imbalance or complex co-occurrence, iterative_by_annotation approach recommended for best balance"
    
    characteristics['recommended_strategy'] = recommended_strategy
    characteristics['recommendation_reason'] = reason
    
    print(f"ğŸ“ˆ Dataset Analysis Results:")
    print(f"   Total images: {characteristics['total_images']:,}")
    print(f"   Total categories: {characteristics['total_categories']:,}")
    print(f"   Imbalance ratio: {imbalance_ratio:.1f}:1")
    print(f"   Avg categories per image: {avg_categories_per_image:.2f}")
    print(f"   Unique combinations: {characteristics['unique_combinations']:,}")
    print(f"\nğŸ’¡ Recommended strategy: {recommended_strategy}")
    print(f"   Reason: {reason}")
    
    return characteristics


def validate_split_quality(stats: Dict[str, Dict[str, int]], 
                          min_val_ratio: float = 0.1,
                          min_test_ratio: float = 0.05) -> Dict[str, bool]:
    """ë¶„í•  í’ˆì§ˆ ê²€ì¦"""
    quality_check = {
        'adequate_validation_size': False,
        'adequate_test_size': False, 
        'no_empty_splits': False,
        'balanced_distribution': False
    }
    
    total_images = sum(split_data['images'] for split_data in stats.values())
    
    if total_images == 0:
        return quality_check
    
    val_ratio = stats.get('val', {}).get('images', 0) / total_images
    test_ratio = stats.get('test', {}).get('images', 0) / total_images
    
    quality_check['adequate_validation_size'] = val_ratio >= min_val_ratio
    quality_check['adequate_test_size'] = test_ratio >= min_test_ratio
    quality_check['no_empty_splits'] = all(split_data.get('images', 0) > 0 
                                          for split_data in stats.values())
    
    # ë¶„í¬ ê· í˜•ì„± ì²´í¬ (trainì´ ë„ˆë¬´ ì¹˜ìš°ì¹˜ì§€ ì•Šì•˜ëŠ”ì§€)
    train_ratio = stats.get('train', {}).get('images', 0) / total_images
    quality_check['balanced_distribution'] = 0.5 <= train_ratio <= 0.9
    
    return quality_check


def _visualize_annotations_on_image(
    image_info: Dict,
    annotations: List[Dict],
    title: str,
    save_path: Path,
    source_image_path: Optional[Path] = None
):
    """ì´ë¯¸ì§€ ìœ„ì— ì–´ë…¸í…Œì´ì…˜ì„ ê·¸ë¦¬ê³  ì €ì¥í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜"""
    try:
        # ì›ë³¸ ì´ë¯¸ì§€ ê²½ë¡œê°€ ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ í° ë°°ê²½ ìƒì„±
        if source_image_path and source_image_path.exists():
            image = cv2.imread(str(source_image_path))
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        else:
            width = image_info['width']
            height = image_info['height']
            image = np.ones((height, width, 3), dtype=np.uint8) * 255
            if not source_image_path:
                print(f"      âš ï¸ Source image not found for {image_info['file_name']}. Drawing on white background.")

        for ann in annotations:
            bbox = ann['bbox']
            x, y, w, h = [int(c) for c in bbox]
            # ë¹¨ê°„ìƒ‰ ì‚¬ê°í˜•ìœ¼ë¡œ ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)

        plt.figure(figsize=(16, 12))
        plt.imshow(image)
        plt.title(f"Distribution of '{title}' in {image_info['file_name']} ({len(annotations)} instances)")
        plt.axis('off')
        plt.savefig(save_path, bbox_inches='tight')
        plt.close()
    except Exception as e:
        print(f"      âŒ Failed to visualize for {image_info['file_name']}: {e}")

if __name__ == "__main__":
    print("Starting comprehensive dataset splitting examples...")
    
    base_dir = Path(os.getcwd()).resolve()
    
    data_path = base_dir / "assets"
    
    # ë°ì´í„°ì…‹ íŠ¹ì„± ë¶„ì„ ë° ì¶”ì²œ
    # characteristics = analyze_dataset_characteristics(data_path / "merged_dataset.json")
    # recommended_strategy = characteristics['recommended_strategy']

    # ì›ë³¸ ì´ë¯¸ì§€ê°€ ìˆëŠ” ë””ë ‰í† ë¦¬ ëª©ë¡
    source_directories = [
        data_path / "TS",
        data_path / "VS",
    ]

    # ì†Œìˆ˜ í´ë˜ìŠ¤ ë¶„í¬ ì‹œê°í™” ë¶„ì„
    # run_rare_class_analysis = input("\nğŸ”¬ Analyze rare class locality and visualize? (y/n): ").lower().strip() == 'y'
    # if run_rare_class_analysis:
    #     # ë¶„ì„ì„ ìœ„í•´ splitter ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ë°ì´í„° ë¡œë“œ
    #     analysis_splitter = create_splitter(input_json=data_path / "merged_dataset.json", output_dir=data_path / "temp_for_analysis")
    #     analyze_rare_class_locality(
    #         splitter=analysis_splitter,
    #         output_dir=data_path / "rare_class_analysis",
    #         source_dirs=source_directories
    #     )
    
    # ë¶„í• 
    # print(f"\nUsing recommended strategy: {recommended_strategy}")
    stats = quick_split(
        input_json=data_path / "merged_v01_prepro.json",
        # input_json=data_path / "merged_dataset.json",
        # output_dir=data_path / f"recommended_split_{recommended_strategy}",
        output_dir=data_path / "strategy_comparison_v01" / "random_split",
        # strategy=recommended_strategy,
        strategy="random",
        train_ratio=0.8,
        val_ratio=0.1,
        image_dir=None
    )
    
    # í’ˆì§ˆ ê²€ì¦
    # quality = validate_split_quality(stats)
    # print(f"Quality Check: {quality}")
    
    # ì—¬ëŸ¬ ì „ëµ ë¹„êµ
    # comparison_results = compare_strategies(
    #     input_json=data_path / "merged_dataset.json",
    #     output_base_dir=data_path / "strategy_comparison"
    # )
    
    
    # # 5. ì‚¬ìš©ì ë§ì¶¤í˜• ì„¤ì • ì˜ˆì‹œ (ê·¹ì‹¬í•œ ë¶ˆê· í˜•ìš©)
    # if characteristics['imbalance_ratio'] > 100:
    #     print(f"Extreme imbalance detected ({characteristics['imbalance_ratio']:.1f}:1)")
    #     print("Creating custom split for extreme imbalance...\n")
        
    #     custom_splitter = create_splitter(
    #         input_json=data_path / "merged_dataset.json",
    #         output_dir=data_path / "extreme_custom_split",
    #         strategy="hybrid",
    #         train_ratio=0.8,  # ë” ë§ì´ trainì— í• ë‹¹
    #         val_ratio=0.15
    #     )
        
    #     custom_splitter.set_strategy_params(
    #         rare_threshold=min(100, max(characteristics['class_distribution'].values()) // 50),
    #         very_rare_threshold=min(20, max(characteristics['class_distribution'].values()) // 200),
    #         min_samples_per_category=2,
    #         min_samples_per_split=1
    #     )
        
    #     custom_stats = custom_splitter.split()
    #     print("Custom extreme imbalance split completed!\n")

    
    
    print(f"All dataset splitting operations completed successfully!")